{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Technical) Jupyter Notebook Standardizing Identifier Order In Adjacent Columns in hu.MAP3-provided CSV\n",
    "\n",
    "Ah, classic data wrangling story....\n",
    "\n",
    "Long story, short, I had assumed in the author-provided CSV the indentifiers listed in the **adjacent** comma-separated column Uniprot_ACCs and column genenames were an in-order match. **They were not.** This effort fixes that in the end because it seems like a good data managemet practice and makes steps in my pipelines easier. Along the way I check maybe what was going on out of curiousity.\n",
    "\n",
    "------\n",
    "\n",
    "**Details**, i.e., the long story:\n",
    "\n",
    "Authors provided a CSV about the hu.MAP 3.0 complex. (See below where I remark, 'I had wanted to get the notebook every time from the source...' for how I had wanted to get that direct for all the Jupyter Notebooks in this related series.)\n",
    "\n",
    "Looking at a few examples I had assumed the **adjacent** comma-separated column Uniprot_ACCs and column genenames were an in-order match. Let's look at some raw rows of the data to see if that was valid I assumed that.\n",
    "\n",
    "Example from the raw data (line 6663):\n",
    "\n",
    "```text\n",
    "huMAP3_06662.1,2,O14832 O75381 Q8WY41,PHYH PEX14 NANOS1\n",
    "```\n",
    "\n",
    "- [O14832](https://www.uniprot.org/uniprotkb/O14832/entry) corresponds to `PHYH`.\n",
    "- [O75381](https://www.uniprot.org/uniprotkb/O75381/entry) corresponds to `PEX14`.\n",
    "- [Q8WY41](https://www.uniprot.org/uniprotkb/Q8WY41/entry) correspinds to `NANOS`.\n",
    "\n",
    "So they are an in-order match in the two columns.  \n",
    "(There's more examples below,under 'Detour to see why mismatching number of identifiers in the two columns, `Uniprot_ACCs` & `genenames`, in 279 rows', where the order seems to match in the order in the two colums.)\n",
    "And that seems good data practice and so I assumed they matched.  \n",
    "WRONG!!!\n",
    "\n",
    "Looking at NHP2 example (line 6379):\n",
    "\n",
    "```text\n",
    "huMAP3_06378.1,2,Q8N8A6 Q9NX24 Q9Y2R4,NHP2 DDX52 DDX51\n",
    "```\n",
    "\n",
    "It turns out there:\n",
    "- [Q8N8A6](https://www.uniprot.org/uniprotkb/Q8N8A6/entry) corresponds to `DDX51`.\n",
    "- [Q9Y2R4](https://www.uniprot.org/uniprotkb/Q9Y2R4/entry) corresponds to `DDX52`.\n",
    "- [Q9NX24](https://www.uniprot.org/uniprotkb/Q9NX24/entry) corresponds to `NHP2`.\n",
    "\n",
    "And so the order doesn't seem to match at all!?!?! The column after the comma doesn't match; teh right column would need to be DDX51, NHP2, DDX52 to match leftc column. (See me questioning what is alphabetical here just below.)\n",
    "\n",
    "That caused issues downstream with extra versions and things not matching appropriately between accession and gene names when I filtered for a unique set as I was using the order to match them up.\n",
    "\n",
    "**This effort fixes/validates that to produce a standardized matching order for the identifiers in the two columns in the end.**   \n",
    "I was already getting the data not from the source  (see below where I remark, 'I had wanted to get the notebook every time from the source...'). So I might as well make it more standardized and easier to use downstream for myself and others.\n",
    "A few reasons rationalizing this:\n",
    "- the step to make the look-up is time consuming (have to look up and sort UniProt KB info all 13769 identifiers and not just the subset complexed with your protein of inters) and so it would have been a pain to fix at the start of a new session everytime.\n",
    "- I had already written code in a few places that assumes an in-order match between the identifiers in the two adjacent columns and so since I have to get the data not from the source, already might was well make it more useable. \n",
    "- standardized in-order match between the data in the two adjacent columns just seems like better data management practice in general.\n",
    "\n",
    "Out of curiosity, along the way to fixing/standardizing I want to see if the sequence of the identifiers not matching was there all along just by chance sampling a small example I happened to think they were in matching order sequence? ..... Was it just data transformation steps gone wrong? .... Are they in alphantical order within each column maybe? .... Corresponding matching order would make more sense for the adjacent but maybe they used Excel along the way it and it transformed things in the process?\n",
    "\n",
    "For the notebooks in this related series stored here in my humap3-binder repo, I had wanted to get the notebook every time from the source so always can say based on what was prvoided but MyBinder seems to block the involved port (maybe?) because for whatever reason I cannot `curl` getting that file from the hu.MAP3 source and so I was storing it in [a gist here](https://gist.githubusercontent.com/fomightez/d84614a9250f13b8d24217b1330c66e3) where MyBinder could access via curl. Turned out in the long run that wouldn't have worked very well anyway because would need time-consuming step of collecting information about identifiers and fixing the adjacent columns so identifiers in column Uniprot_ACCs and column genenames were an in-order match everytime. So I'm just going to fix. This document will also serve as my way to record these steps to fix too, and in that way if the investigatos release an update or a highly related CSV file, I can run this again to validate or fix as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick check, is it alphabetical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huMAP3_06137.1,2,Q8N475 Q9P2F8 Q9Y250,LZTS1 FSTL5 SIPA1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q8N8A6', 'Q9NX24', 'Q9Y2R4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw data Q8N8A6 Q9NX24 Q9Y2R4,NHP2 DDX52 DDX51\n",
    "mylist = [\"Q8N8A6\", \"Q9NX24\", \"Q9Y2R4\"]\n",
    "mylist.sort()\n",
    "mylist # alphabetical: `['Q8N8A6', 'Q9NX24', 'Q9Y2R4']` and so those accessions in data seem alphabetical; other side would neex to be DDX51, NHP2, DDX52 to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DDX51', 'DDX52', 'NHP2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [\"NHP2\", \"DDX52\", \"DDX51\"]\n",
    "mylist.sort()\n",
    "mylist # Looks like alphabetical here would be ['DDX51', 'DDX52', 'NHP2'] so they aren't alphabetical under `genenames` themselves, not clear at all why `genenames` in that order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Q8N8A6](https://www.uniprot.org/uniprotkb/Q8N8A6/entry) corresponds to `DDX51`.\n",
    "- [Q9Y2R4](https://www.uniprot.org/uniprotkb/Q9Y2R4/entry) corresponds to `DDX52`.\n",
    "- [Q9NX24](https://www.uniprot.org/uniprotkb/Q9NX24/entry) corresponds to `NHP2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the complexes with confidence scores\n",
    "\n",
    "While `curl -OL \"https://humap3.proteincomplexes.org/static/downloads/humap3/hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922.csv\"` works on my local machine, the involved port may be blocked on MyBinder for getting it from the original resource.  \n",
    "I suspect this won't change much and so for now I am getitng it from a copy I made from the original source. So if you find something interesting & want to confirm, get the newest data and replace this file by drag and dropping from your local machine into the file navigation paenl on the right. and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1241k  100 1241k    0     0  2190k      0 --:--:-- --:--:-- --:--:-- 2189k\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://gist.githubusercontent.com/fomightez/d84614a9250f13b8d24217b1330c66e3/raw/d2d6cb846ba22c1f9f3a4fb36002aa4ce9ba621f/hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Put the data on the complexes into Pandas dataframe\n",
    "\n",
    "(I'm using uv here just because I want to learn about it. I could have run the code in the script right in this notebook, and skipped the pickling and read pickle steps.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the script to use with `uv` to read in the raw data and make a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1007  100  1007    0     0   3289      0 --:--:-- --:--:-- --:--:--  3290\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/structurework/refs/heads/master/humap3-utilities/complexes_rawCSV_to_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inline script metadata from `\u001b[36mcomplexes_rawCSV_to_df.py\u001b[39m`\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP3_ID</th>\n",
       "      <th>ComplexConfidence</th>\n",
       "      <th>Uniprot_ACCs</th>\n",
       "      <th>genenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huMAP3_00000.1</td>\n",
       "      <td>1</td>\n",
       "      <td>P20963 Q9NWV4 Q9UGQ2</td>\n",
       "      <td>CD247 CACFD1 CZIB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huMAP3_00001.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O94887 Q9NQ92 Q9NWB1</td>\n",
       "      <td>FARP2 COPRS RBFOX1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huMAP3_00002.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q8N3D4 Q9Y3A4</td>\n",
       "      <td>RRP7A EHBP1L1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huMAP3_00003.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O00429 Q5T2D2</td>\n",
       "      <td>DNM1L TREML2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huMAP3_00004.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O95460 P21941 P78540 Q9H267 Q9H9C1</td>\n",
       "      <td>MATN4 MATN1 ARG2 VPS33B VIPAS39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15321</th>\n",
       "      <td>huMAP3_15345.1</td>\n",
       "      <td>6</td>\n",
       "      <td>O14628 Q3SXZ3</td>\n",
       "      <td>ZNF195 ZNF718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15322</th>\n",
       "      <td>huMAP3_15346.1</td>\n",
       "      <td>6</td>\n",
       "      <td>P08910 Q6ZWT7 Q86VD1 Q9UJQ1 Q9Y6X9</td>\n",
       "      <td>ABHD2 MBOAT2 MORC1 LAMP5 MORC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15323</th>\n",
       "      <td>huMAP3_15347.1</td>\n",
       "      <td>6</td>\n",
       "      <td>A6ND91 Q4V339</td>\n",
       "      <td>ZNG1F ASPDH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15324</th>\n",
       "      <td>huMAP3_15348.1</td>\n",
       "      <td>6</td>\n",
       "      <td>A6NKF2 P08217 Q8IVW6 Q99856</td>\n",
       "      <td>CELA2A ARID3B ARID3A ARID3C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325</th>\n",
       "      <td>huMAP3_15349.1</td>\n",
       "      <td>6</td>\n",
       "      <td>P51888 Q5SRH9 Q9NRD9 Q9UHX1</td>\n",
       "      <td>PRELP DUOX1 PUF60 TTC39A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15326 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            HuMAP3_ID  ComplexConfidence                        Uniprot_ACCs  \\\n",
       "0      huMAP3_00000.1                  1                P20963 Q9NWV4 Q9UGQ2   \n",
       "1      huMAP3_00001.1                  1                O94887 Q9NQ92 Q9NWB1   \n",
       "2      huMAP3_00002.1                  1                       Q8N3D4 Q9Y3A4   \n",
       "3      huMAP3_00003.1                  1                       O00429 Q5T2D2   \n",
       "4      huMAP3_00004.1                  1  O95460 P21941 P78540 Q9H267 Q9H9C1   \n",
       "...               ...                ...                                 ...   \n",
       "15321  huMAP3_15345.1                  6                       O14628 Q3SXZ3   \n",
       "15322  huMAP3_15346.1                  6  P08910 Q6ZWT7 Q86VD1 Q9UJQ1 Q9Y6X9   \n",
       "15323  huMAP3_15347.1                  6                       A6ND91 Q4V339   \n",
       "15324  huMAP3_15348.1                  6         A6NKF2 P08217 Q8IVW6 Q99856   \n",
       "15325  huMAP3_15349.1                  6         P51888 Q5SRH9 Q9NRD9 Q9UHX1   \n",
       "\n",
       "                             genenames  \n",
       "0                    CD247 CACFD1 CZIB  \n",
       "1                   FARP2 COPRS RBFOX1  \n",
       "2                        RRP7A EHBP1L1  \n",
       "3                         DNM1L TREML2  \n",
       "4      MATN4 MATN1 ARG2 VPS33B VIPAS39  \n",
       "...                                ...  \n",
       "15321                    ZNF195 ZNF718  \n",
       "15322   ABHD2 MBOAT2 MORC1 LAMP5 MORC2  \n",
       "15323                      ZNG1F ASPDH  \n",
       "15324      CELA2A ARID3B ARID3A ARID3C  \n",
       "15325         PRELP DUOX1 PUF60 TTC39A  \n",
       "\n",
       "[15326 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!uv run complexes_rawCSV_to_df.py hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922.csv\n",
    "import pandas as pd\n",
    "rd_df = pd.read_pickle('raw_complexes_pickled_df.pkl')\n",
    "rd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial idea to make in-order was to collect the identifiers in the Uniprot_ACCs & genenames columns by exploding the columns **together** and then making a list that is reduced to just the unique set. Then that would serve as a basis making a lookup table to relate the contents of the two columns.    \n",
    "But that doesn't work for both columns together as I had discovered, & cover next the reasoning, and so the code is left with what works and that is just 'exploding' one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75531"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can use Pandas `explode()`-related code I worked out in relation to using `rd_df` data already (see `notebooks/Working_with_hu.MAP3_data_with_Python_in_Jupyter_Basics.ipynb`)\n",
    "# to make the step of flattening the Uniprot_ACCs\n",
    "intermed_df = rd_df.copy()\n",
    "intermed_df['Uniprot_ACCs'] = intermed_df['Uniprot_ACCs'].str.split()\n",
    "#intermed_df['genenames'] = intermed_df['genenames'].str.split() # cannot use explode with both columns because out of 15326 rows, 279 rows don't have same number of indentifiers in the two columns (?!?! Do some identifiers occur twice in one or other?)\n",
    "# Now use explode to create a new row for each element in both columns\n",
    "expanded_df = intermed_df.explode(['Uniprot_ACCs']).copy()\n",
    "# Reset the index \n",
    "expanded_df = expanded_df.reset_index(drop=True)\n",
    "len(expanded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work as I had thought for both columns, read on for as to why and how fixed so ultimately exploding those two columns together will work (since the code I made made elsewhere and used was assuming 'exploding' the columns would work well)....\n",
    "\n",
    "#### Detour to see why mismatching number of identifiers in the two columns, `Uniprot_ACCs` & `genenames`, in 279 rows\n",
    "\n",
    "Noted something curious while drafting the code for the above cell..\n",
    "\n",
    "Found couldn't use `explode()` with both columns of the entire 'raw data' dataframe because out of 15326 rows, 279 rows (see next cell showing this) don't have same number of indentifiers in the two columns (?!?! Do some identifiers occur twice in one or other?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total rows in the CSV: <span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">15326</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total rows in the CSV: \u001b[1;30m15326\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Total Rows where the number of identifiers in the\n",
       "two adjacent columns aren't the same: <span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">279</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Total Rows where the number of identifiers in the\n",
       "two adjacent columns aren't the same: \u001b[1;30m279\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intermed_df = rd_df.copy()\n",
    "intermed_df['Uniprot_ACCs'] = intermed_df['Uniprot_ACCs'].str.split()\n",
    "intermed_df['genenames'] = intermed_df['genenames'].str.split()\n",
    "#intermed_df.head()\n",
    "num = 0\n",
    "for row in intermed_df.itertuples():\n",
    "    if len(row.Uniprot_ACCs) != len(row.genenames):\n",
    "        num+=1\n",
    "import rich\n",
    "rich.print(f\"Total rows in the CSV: [bold black]{len(intermed_df)}[/bold black]\")\n",
    "rich.print(f\"Total Rows where the number of identifiers in the\\ntwo adjacent columns aren't the same: [bold black]{num}[/bold black]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some examples that can be explored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP3_ID</th>\n",
       "      <th>ComplexConfidence</th>\n",
       "      <th>Uniprot_ACCs</th>\n",
       "      <th>genenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13129</th>\n",
       "      <td>huMAP3_13145.1</td>\n",
       "      <td>6</td>\n",
       "      <td>[Q5TCX8, Q9BQ83]</td>\n",
       "      <td>[MAP3K21, SLX1A;, SLX1B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>huMAP3_05650.1</td>\n",
       "      <td>2</td>\n",
       "      <td>[P78423, Q8IVW1, Q8NHU3]</td>\n",
       "      <td>[CX3CL1, SGMS2, ARL17A;, ARL17B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7576</th>\n",
       "      <td>huMAP3_07577.1</td>\n",
       "      <td>2</td>\n",
       "      <td>[P68431, Q8N806]</td>\n",
       "      <td>[H3C1;, H3C2;, H3C3;, H3C4;, H3C6;, H3C7;, H3C8;, H3C10;, H3C11;, H3C12, UBR7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6588</th>\n",
       "      <td>huMAP3_06589.1</td>\n",
       "      <td>2</td>\n",
       "      <td>[A0A087X2D5, B2CML4, O14561, O75394, O95900, P09001, P49406, P52815, P79522, P82663, Q13084, Q13405, Q13595, Q14197, Q15070, Q15434, Q16540, Q1KMD3, Q4U2R6, Q5T653, Q6P087, Q6P161, Q6P1L8, Q6XE24, Q7Z7F7, Q7Z7H8, Q86TS9, Q8IUH3, Q8IXM3, Q8IYB8, Q8N0T1, Q8N5N7, Q8N983, Q8TAE8, Q8TCC3, Q969S9, Q96A35, Q96CM3, Q96DH6, Q96DV4, Q96EH3, Q96EL3, Q96GC5, Q96I24, Q96KR1, Q96SI9, Q9BQ48, Q9BQC6, Q9BYC8, Q9BYC9, Q9BYD1, Q9BYD2, Q9BYD3, Q9BYD6, Q9BYK8, Q9BZE1, Q9H0U6, Q9H2W6, Q9H9J2, Q9HC36, Q9HD33, Q9NP92, Q9NPE2, Q9NQ50, Q9NRX2, Q9NUL7, Q9NVS2, Q9NWU5, Q9NX20, Q9NXV6, Q9NYK5, Q9NYY8, Q9NZE8, Q9P015, Q9P0M9, Q9UBX3, Q9Y2Z4, Q9Y3B7, Q9Y6G3]</td>\n",
       "      <td>[NDUFAB1, TRUB2, MRPL12, PRR3, MRPL28, MRPL49, TRA2A, MRPL58, OXA1L, MRPL23, RPUSD3, RBMS3, MRPL10, MRPL52, RBM45, MRPL41, SUPV3L1, MRPL43, GADD45GIP1, GFM2, RPUSD4, MSI2, MRPL38, MALSU1, MRPL53, FUBP3, STRBP, HELZ2, MRPL18, MRPL44, MRM3, NGRN, MRPL40, DDX28, MRPL22, CDKN2AIP, MRPL39, FASTKD2, MRPL35, SLC25A10, YARS2, MRPL11, MRPL42, MRPL33, MRPL3, MRPL19, MRPS25, MRPL51, MRPL2, MRPL14, MRPL55, RBIS, MRPL50, MRPL30, MRPL24, MRPL48, ZFR, MRPL57, MRPL32, MRPL20, MRPL13, MRPL9, MRPL4, MRPL37, MRPL46, MRPL47, MRPS30, MRPL17, MRPS18A, MRPL16, MRPL15, MRPL27, RBMS2, HNRNPUL2, MRPL54, MRPL34, MRPL1, MRPL45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>huMAP3_06036.1</td>\n",
       "      <td>2</td>\n",
       "      <td>[P07357, P32246, P49441, Q71DI3, Q9H6B9, Q9H720]</td>\n",
       "      <td>[C8A, CCR1, H3C15;, H3C14;, H3C13, EPHX3, INPP1, CWH43]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6271</th>\n",
       "      <td>huMAP3_06272.1</td>\n",
       "      <td>2</td>\n",
       "      <td>[Q8WV35, Q9NZ71]</td>\n",
       "      <td>[RTEL1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12397</th>\n",
       "      <td>huMAP3_12412.1</td>\n",
       "      <td>6</td>\n",
       "      <td>[P28331, Q96IZ6, Q96LI6, Q96NC0, Q9NUJ1]</td>\n",
       "      <td>[NDUFS1, METTL2A, HSFY1;, HSFY2, ZMAT2, ABHD10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9321</th>\n",
       "      <td>huMAP3_09326.1</td>\n",
       "      <td>4</td>\n",
       "      <td>[P49247, Q86VZ2, Q8NHG8, Q96JG8]</td>\n",
       "      <td>[RPIA, ZNRF2, MAGED4;, MAGED4B, WDR5B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14563</th>\n",
       "      <td>huMAP3_14582.1</td>\n",
       "      <td>6</td>\n",
       "      <td>[645345, O75596, Q8NDY6]</td>\n",
       "      <td>[CLEC3A, BHLHE23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>huMAP3_01702.1</td>\n",
       "      <td>1</td>\n",
       "      <td>[Q96PE3, Q9GZY0, Q9H4D5, Q9NPJ8]</td>\n",
       "      <td>[INPP4A, NXF2;, NXF2B, NXF3, NXT2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            HuMAP3_ID  ComplexConfidence  \\\n",
       "13129  huMAP3_13145.1                  6   \n",
       "5650   huMAP3_05650.1                  2   \n",
       "7576   huMAP3_07577.1                  2   \n",
       "6588   huMAP3_06589.1                  2   \n",
       "6035   huMAP3_06036.1                  2   \n",
       "6271   huMAP3_06272.1                  2   \n",
       "12397  huMAP3_12412.1                  6   \n",
       "9321   huMAP3_09326.1                  4   \n",
       "14563  huMAP3_14582.1                  6   \n",
       "1702   huMAP3_01702.1                  1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Uniprot_ACCs  \\\n",
       "13129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              [Q5TCX8, Q9BQ83]   \n",
       "5650                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [P78423, Q8IVW1, Q8NHU3]   \n",
       "7576                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [P68431, Q8N806]   \n",
       "6588   [A0A087X2D5, B2CML4, O14561, O75394, O95900, P09001, P49406, P52815, P79522, P82663, Q13084, Q13405, Q13595, Q14197, Q15070, Q15434, Q16540, Q1KMD3, Q4U2R6, Q5T653, Q6P087, Q6P161, Q6P1L8, Q6XE24, Q7Z7F7, Q7Z7H8, Q86TS9, Q8IUH3, Q8IXM3, Q8IYB8, Q8N0T1, Q8N5N7, Q8N983, Q8TAE8, Q8TCC3, Q969S9, Q96A35, Q96CM3, Q96DH6, Q96DV4, Q96EH3, Q96EL3, Q96GC5, Q96I24, Q96KR1, Q96SI9, Q9BQ48, Q9BQC6, Q9BYC8, Q9BYC9, Q9BYD1, Q9BYD2, Q9BYD3, Q9BYD6, Q9BYK8, Q9BZE1, Q9H0U6, Q9H2W6, Q9H9J2, Q9HC36, Q9HD33, Q9NP92, Q9NPE2, Q9NQ50, Q9NRX2, Q9NUL7, Q9NVS2, Q9NWU5, Q9NX20, Q9NXV6, Q9NYK5, Q9NYY8, Q9NZE8, Q9P015, Q9P0M9, Q9UBX3, Q9Y2Z4, Q9Y3B7, Q9Y6G3]   \n",
       "6035                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [P07357, P32246, P49441, Q71DI3, Q9H6B9, Q9H720]   \n",
       "6271                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [Q8WV35, Q9NZ71]   \n",
       "12397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [P28331, Q96IZ6, Q96LI6, Q96NC0, Q9NUJ1]   \n",
       "9321                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [P49247, Q86VZ2, Q8NHG8, Q96JG8]   \n",
       "14563                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [645345, O75596, Q8NDY6]   \n",
       "1702                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [Q96PE3, Q9GZY0, Q9H4D5, Q9NPJ8]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             genenames  \n",
       "13129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [MAP3K21, SLX1A;, SLX1B]  \n",
       "5650                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [CX3CL1, SGMS2, ARL17A;, ARL17B]  \n",
       "7576                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    [H3C1;, H3C2;, H3C3;, H3C4;, H3C6;, H3C7;, H3C8;, H3C10;, H3C11;, H3C12, UBR7]  \n",
       "6588   [NDUFAB1, TRUB2, MRPL12, PRR3, MRPL28, MRPL49, TRA2A, MRPL58, OXA1L, MRPL23, RPUSD3, RBMS3, MRPL10, MRPL52, RBM45, MRPL41, SUPV3L1, MRPL43, GADD45GIP1, GFM2, RPUSD4, MSI2, MRPL38, MALSU1, MRPL53, FUBP3, STRBP, HELZ2, MRPL18, MRPL44, MRM3, NGRN, MRPL40, DDX28, MRPL22, CDKN2AIP, MRPL39, FASTKD2, MRPL35, SLC25A10, YARS2, MRPL11, MRPL42, MRPL33, MRPL3, MRPL19, MRPS25, MRPL51, MRPL2, MRPL14, MRPL55, RBIS, MRPL50, MRPL30, MRPL24, MRPL48, ZFR, MRPL57, MRPL32, MRPL20, MRPL13, MRPL9, MRPL4, MRPL37, MRPL46, MRPL47, MRPS30, MRPL17, MRPS18A, MRPL16, MRPL15, MRPL27, RBMS2, HNRNPUL2, MRPL54, MRPL34, MRPL1, MRPL45]  \n",
       "6035                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [C8A, CCR1, H3C15;, H3C14;, H3C13, EPHX3, INPP1, CWH43]  \n",
       "6271                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [RTEL1]  \n",
       "12397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [NDUFS1, METTL2A, HSFY1;, HSFY2, ZMAT2, ABHD10]  \n",
       "9321                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [RPIA, ZNRF2, MAGED4;, MAGED4B, WDR5B]  \n",
       "14563                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [CLEC3A, BHLHE23]  \n",
       "1702                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [INPP4A, NXF2;, NXF2B, NXF3, NXT2]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter rows where list lengths differ\n",
    "i_df = rd_df.copy()\n",
    "i_df['Uniprot_ACCs'] = i_df['Uniprot_ACCs'].str.split()\n",
    "i_df['genenames'] = i_df['genenames'].str.split()\n",
    "mismtchd = i_df[i_df['Uniprot_ACCs'].apply(len) != i_df['genenames'].apply(len)]\n",
    "\n",
    "# Randomly select 10 rows from the filtered DataFrame\n",
    "sample_rows = mismtchd.sample(n=10, random_state=3) #use state to assure sampling is same when re-run\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "    display(sample_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice sampling of examples to examine.\n",
    "\n",
    "Let's look at a few:\n",
    "\n",
    "Example from row #14563: `[645345, O75596, Q8NDY6]\t[CLEC3A, BHLHE23]`\n",
    "\n",
    "That first UniProt accession, `645345`, doesn't even match the format of typical UniProt accension. (`O75596` corresponds to `CLEC3A` and `Q8NDY6` corresponds to `BHLHE23`.) So it makes it hard to to tell what may be going on here since the two out of three under 'Uniprot_ACCs' are both accounted for in the genenames column. (I even looked at what the other 11 of 12 rows where `645345` occurs and I couldn't account for it from the rows where there was a manageable number to try and deconvolute the situation.)   \n",
    "UPDATE: by searching associated proteins at the later I found [the NCBI link for this](https://www.ncbi.nlm.nih.gov/gene/645345) (the UniProt link they provided was a dead end) at the authors'-provided site for querying the complexes non-programmatically. That site links to [the HGNC enetry for this protein/gene](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/HGNC:23676) that has a UniProt entry, [Q5T1J5](https://www.uniprot.org/uniprotkb/Q5T1J5/entry). So why not use [Q5T1J5](https://www.uniprot.org/uniprotkb/Q5T1J5/entry) to be consistent with all the other **13768** protein identifiers in that column? Handled differently because putative?\n",
    "\n",
    "Example from row #6271: `[Q8WV35, Q9NZ71]\t[RTEL1]`\n",
    "If I look up `RTEL1` at UniProt, I see [Q9NZ71](https://www.uniprot.org/uniprotkb/Q9NZ71/entry).\n",
    "If I search `Q8WV35` at UniProt, I get [this](https://www.uniprot.org/uniprotkb/Q8WV35/history) where says:\n",
    "\n",
    "```text\n",
    "This entry is no longer annotated in UniProtKB and can be found in UniParc.\n",
    "Reason: Deleted from Swiss-Prot\n",
    "Since release: 2022_05/2022_05\n",
    "```\n",
    "\n",
    "(There's also an 'i' information indicating symbol after the `UniParc` text there that leads to a pop-up, see 'the information in the info-pop up' below.) \n",
    "Below is a table with the last entry 'Release Date' column at the top from 03-Aug-2022 and if I click `txt` next to `153` below 'Entry Version' column (first column) along that row, I end up at [here](\n",
    "https://rest.uniprot.org/unisave/Q8WV35?format=txt&versions=153) where it has what looks to be a GENBANK like entry with the second line `AC   Q8WV35; B2RE92; Q9UKA0;` and the tenth line `GN   Name=LRRC29; Synonyms=FBL9, FBXL9;`\n",
    "\n",
    "Searching that 'LRRC29' or 'FBXL9' in HGNC gets me to [here](https://genenames.org/data/gene-symbol-report/#!/hgnc_id/HGNC:13605) where it says FBXL9P is a pseudogene.\n",
    "\n",
    "That is consistent with the information in the info-pop up after UniParc that I mentioned above that says:\n",
    "\n",
    ">\"Deleted entries in UniProtKB/Swiss-Prot are mostly Open Reading Frames (ORFs) or pseudogenes that have been wrongly predicted to code for proteins. .... Most UniProtKB/TrEMBL deletions are due to the deletion of the corresponding coding sequence (CDS) in the source nucleotide sequence databases EMBL-Bank/DDBJ/GenBank as requested by the original submitters, or due to the deletion of the sequence prediction from Ensembl or RefSeq. ... In addition, some protein sequences are recognized by curators to be Open Reading frames (ORFs) that have been wrongly predicted to code for proteins or to be pseudogenes. When there is enough evidence that these hypothetical proteins are not real, we take the decision to remove them from UniProtKB/TrEMBL.\"\n",
    "\n",
    "I guess the hu.MAP 3.0 data would argue it shouldn't have been deleted and this is the way they have of still including it, but why not in genenames?\n",
    "\n",
    "So that would mean most would be expected to have more under Uniprot_ACCs if that shows psuedogenes and that general trend does seem to be the case....\n",
    "But then what about example from row #13129: `[Q5TCX8, Q9BQ83]\t[MAP3K21, SLX1A;, SLX1B]`\n",
    "[Q5TCX8](https://www.uniprot.org/uniprotkb/Q5TCX8/entry) corresponds to `MAP3K21` and [Q9BQ83](https://www.uniprot.org/uniprotkb/Q9BQ83/entry) corresponds to `SLX1A; SLX1B`. I get why that one doesn't match. In that case the semi-colon is there to signify it has two names. (Keep in mind the original row there in the data CSV file looks like `huMAP3_13145.1,6,Q5TCX8 Q9BQ83,MAP3K21 SLX1A; SLX1B` because what is represented here is a Python list made from that content.)\n",
    "\n",
    "So for row #12397 there is a case with a semi-colon, too:\n",
    "\n",
    "```text\n",
    "[P28331, Q96IZ6, Q96LI6, Q96NC0, Q9NUJ1]\t[NDUFS1, METTL2A, HSFY1;, HSFY2, ZMAT2, ABHD10]\n",
    "```\n",
    "\n",
    "Does the way of writing the two name gene account for that there? Accounting for cheking this:\n",
    "\n",
    "- [P28331](https://www.uniprot.org/uniprotkb/P28331/entry) corresponds to `NDUFS1`.\n",
    "- [Q96IZ6](https://www.uniprot.org/uniprotkb/Q96IZ6/entry) corresponds to `METTL2A`.\n",
    "- [Q96LI6](https://www.uniprot.org/uniprotkb/Q96LI6/entry) corresponds to `HSFY1; HSFY2`.\n",
    "\n",
    "So the semi-colons seem to explain where more identifiers on the side of the `genenames` column.  \n",
    "Later found at least two cases where there's three genenames, like `F8A1;F8A2;F8A3` and `TEX28;TEX28P1;TEX28P2`, per UniProt identifier.\n",
    "\n",
    "Related observational take-away from looking at these cases that relates to below:  \n",
    "The ones I investigated here, the position of the identifier on the `Uniprot_ACCs` column corresponds to the order in the `genenames` colunn. So this would argue for in-order matched and maybe I wasn't offbase to conclude that earlier. (But I have learned I definitely should have looked more intently!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make lookup table for relating identifiers in the two columns, column Uniprot_ACCs and column genenames\n",
    "\n",
    "Will look up the Uniprot_ACCs indentifiers in the UniProt Knowledgebase using the package Unipressed (see my [Unipressed-binder repo](https://github.com/fomightez/Unipressed-binder) for more on working with this package). Note that I will not use the ID mapping to HGNC because for one thing, `request = IdMappingClient.submit(source=\"UniProtKB_AC-ID\", dest=\"HGNC\", ids=id_list)` , gives results like `[{'from': 'Q5VV41', 'to': 'HGNC:15515'}]`. And so additional work would be needed to go to gene name there. Secondly, I know from looking, e.g., `SLX1A; SLX1B`. At HGNC, it is just `SLX1A` [there](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/HGNC:20922) and a separate entry for `SLX1B` [there](https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/HGNC:28748), even though listed as same locus in both entries. Importantly, `SLX1B` is not on the `SLX1A` HGNC page, except I see `Slx1b` listed under moust ortholog, and so would be hard to expect to easily relate these types of things to what is used in the hu.MAP 3.0 data. I could though use `request = IdMappingClient.submit(source=\"UniProtKB_AC-ID\", dest=\"GeneCards\", ids=id_list)` and for `Q9BQ83` that returns two items: `[{'from': 'Q9BQ83', 'to': 'SLX1A'},{'from': 'Q9BQ83', 'to': 'SLX1B'}]`. However, while it doesn't return an HTTP error when you give it  'bad' ID, it discards anything where there is no match and so I miss being able to handle that as encountered.\n",
    "\n",
    "First to get started creating that lookup, the next cell will make a list of all the unique identifiers in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13769"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first make a list of all the identifiers in the 'Uniprot_ACCs' column\n",
    "ids_in_Uniprot_ACCs = list(set(expanded_df['Uniprot_ACCs'].to_list())) # the set will limit to unique set, eliminating ones that appear more than once\n",
    "len(ids_in_Uniprot_ACCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 13769 unique identifiers among all the identifiers in the 'Uniprot_ACCs' column. (If I do just over a second for each one to avoid slamming the site, that is 3 hours and 50 minutes. And so I should design this collecint to be able to restartable and do in chunks.)\n",
    "\n",
    "Let's store that (or read back in, or essentially just skip over this cell, depending on situation) for getting back up and running during development optionally faster, without running all cells above again. (Otherwise, in the way the code is in the cell, the cell runs fine without doing anything in present form.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if need to read in the list of all 13769 uniue Uniprot_ACCs\n",
    "'''\n",
    "import pickle\n",
    "with open(\"list_all_13769_uniue_Uniprot_ACCs.pkl\", \"rb\") as f:\n",
    "        ids_in_Uniprot_ACCs = pickle.load(f)\n",
    "len(ids_in_Uniprot_ACCs)\n",
    "'''\n",
    "\n",
    "# if need to save the list of all 13769 uniue Uniprot_ACCs again for some reason\n",
    "'''\n",
    "import pickle\n",
    "with open(\"list_all_13769_uniue_Uniprot_ACCs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ids_in_Uniprot_ACCs , f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13769"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"list_all_13769_uniue_Uniprot_ACCs.pkl\", \"rb\") as f:\n",
    "        ids_in_Uniprot_ACCs = pickle.load(f)\n",
    "len(ids_in_Uniprot_ACCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with identifiers collected and in active memory, collect the gene names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first cell sets things up so can run in chunks to progressively get to complete lookup dictionary eventually\n",
    "from unipressed import UniprotkbClient\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def process_uniprot_chunk(id_list):\n",
    "    \"\"\"\n",
    "    Process a chunk of UniProt IDs and return a dictionary of ID to gene name mappings.\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    for uniprot_id in id_list:\n",
    "        try:\n",
    "            uniprot_record = UniprotkbClient.fetch_one(uniprot_id)\n",
    "            #lookup_dict[uniprot_id] = uniprot_record['genes'][0]['geneName']['value'] #worked for when one 'gene' in the list returned by `uniprot_record['genes']`\n",
    "            # HOWEVER...\n",
    "            # some IDs like `Q96LI6` and `Q9BQ83` give more than one gene for `uniprot_record['genes']` and the CSV from the hu.MAP3 people was combining \n",
    "            # those to things like `HSFY1; HSFY2` and `SLX1A; SLX1B`, respectively. To do accurate accounting and keep close to that (or at least keep the option to keep close to that), I want to recapitulate that, too.\n",
    "            if 'genes' in uniprot_record:\n",
    "                chunk_dict[uniprot_id] = '; '.join([x['geneName']['value'] for x in uniprot_record['genes']])\n",
    "            else:\n",
    "                if uniprot_id == 'B3KT37': # special case I looked into\n",
    "                    chunk_dict[uniprot_id] = 'SPECIALin_UniProt_and_VETELKLIC_part_similar_to_YWHAE_but_no_official_gene'\n",
    "                else:\n",
    "                    chunk_dict[uniprot_id] = 'SPECIALin_UniProt_but_no_gene'\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 400:\n",
    "                chunk_dict[uniprot_id] = 'not_known'\n",
    "                print(f\"UniProt ID '{uniprot_id}' not found. Marked as 'not_known' in lookup_dict.\\n(Although for known case, '645345', this will be fixed later.)\")\n",
    "            else:\n",
    "                raise e\n",
    "        time.sleep(1.12)\n",
    "    return chunk_dict\n",
    "\n",
    "def process_all_ids_in_chunks(id_list, chunk_size=1000, output_dir='chunk_pickles', resume=True):\n",
    "    \"\"\"\n",
    "    Process all IDs in chunks and save each chunk as a pickle file.\n",
    "    Supports resuming from the last completed chunk.\n",
    "    \n",
    "    Args:\n",
    "        id_list (list): List of all UniProt IDs to process\n",
    "        chunk_size (int): Size of each chunk\n",
    "        output_dir (str): Directory to save pickle files\n",
    "        resume (bool): If True, skip already processed chunks\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_chunks = (len(id_list) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    # Find the last completed chunk if resuming\n",
    "    existing_chunks = []\n",
    "    if resume:\n",
    "        existing_chunks = [f for f in os.listdir(output_dir) if f.startswith('chunk_') and f.endswith('.pkl')]\n",
    "        existing_chunks.sort()  # Sort to find the highest number\n",
    "    \n",
    "    # Determine starting chunk\n",
    "    start_chunk = 0\n",
    "    if existing_chunks:\n",
    "        last_chunk = existing_chunks[-1]\n",
    "        start_chunk = int(last_chunk.split('_')[1].split('.')[0])  # Extract number from 'chunk_003.pickle'\n",
    "        print(f\"Resuming from chunk {start_chunk + 1}\")\n",
    "    \n",
    "    # Process remaining chunks\n",
    "    for i in range(start_chunk * chunk_size, len(id_list), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = id_list[i:i + chunk_size]\n",
    "        \n",
    "        pickle_filename = os.path.join(output_dir, f'chunk_{chunk_num:03d}.pkl')\n",
    "        \n",
    "        # Skip if chunk already exists and we're resuming\n",
    "        if resume and os.path.exists(pickle_filename):\n",
    "            print(f\"Skipping existing chunk {chunk_num}/{total_chunks}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Processing chunk {chunk_num}/{total_chunks} ({len(chunk)} IDs)\")\n",
    "        chunk_result = process_uniprot_chunk(chunk)\n",
    "        \n",
    "        with open(pickle_filename, 'wb') as f:\n",
    "            pickle.dump(chunk_result, f)\n",
    "        \n",
    "        print(f\"Saved {len(chunk_result)} entries to {pickle_filename}\")\n",
    "\n",
    "def combine_pickle_chunks(pickle_dir='chunk_pickles'):\n",
    "    \"\"\"\n",
    "    Combine all pickle files in the directory into a single dictionary.\n",
    "    \n",
    "    Args:\n",
    "        pickle_dir (str): Directory containing the pickle files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Combined dictionary of all chunks\n",
    "    \"\"\"\n",
    "    combined_dict = {}\n",
    "    pickle_files = sorted([f for f in os.listdir(pickle_dir) if f.endswith('.pkl')])\n",
    "    \n",
    "    for pickle_file in pickle_files:\n",
    "        with open(os.path.join(pickle_dir, pickle_file), 'rb') as f:\n",
    "            chunk_dict = pickle.load(f)\n",
    "            combined_dict.update(chunk_dict)\n",
    "        print(f\"Loaded {pickle_file}, combined dictionary now has {len(combined_dict)} entries\")\n",
    "    \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from chunk 24\n",
      "Processing chunk 24/28 (500 IDs)\n",
      "Saved 500 entries to chunk_pickles/chunk_024.pkl\n",
      "Processing chunk 25/28 (500 IDs)\n",
      "Saved 500 entries to chunk_pickles/chunk_025.pkl\n",
      "Processing chunk 26/28 (500 IDs)\n",
      "Saved 500 entries to chunk_pickles/chunk_026.pkl\n",
      "Processing chunk 27/28 (500 IDs)\n",
      "Saved 500 entries to chunk_pickles/chunk_027.pkl\n",
      "Processing chunk 28/28 (269 IDs)\n",
      "Saved 269 entries to chunk_pickles/chunk_028.pkl\n"
     ]
    }
   ],
   "source": [
    "# This will take a LONG TIME TO run with all 13769 unique identifiers and so when developing use on a subset. Next line would be uncommented for that.\n",
    "# When actually running, will do in chunks and CAN RESUME with initial chunks (as long as all you same `chunk_size` setting; see `ABOUT RESUMING:` below).\n",
    "#ids_in_Uniprot_ACCs = ids_in_Uniprot_ACCs[:2999] #Only uncommented during development to limit to first ~3K!!\n",
    "#ids_in_Uniprot_ACCs = ['Q96LI6'] # FOR DEBUGGING, example of one with two genenames\n",
    "process_all_ids_in_chunks(ids_in_Uniprot_ACCs, chunk_size=500)  # ABOUT RESUMING: Can automatically resume from chunk 4 if you make a directory `chunk_pickles` in current working directory & drop in the first three there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunk_001.pkl, combined dictionary now has 500 entries\n",
      "Loaded chunk_002.pkl, combined dictionary now has 1000 entries\n",
      "Loaded chunk_003.pkl, combined dictionary now has 1500 entries\n",
      "Loaded chunk_004.pkl, combined dictionary now has 2000 entries\n",
      "Loaded chunk_005.pkl, combined dictionary now has 2500 entries\n",
      "Loaded chunk_006.pkl, combined dictionary now has 3000 entries\n",
      "Loaded chunk_007.pkl, combined dictionary now has 3500 entries\n",
      "Loaded chunk_008.pkl, combined dictionary now has 4000 entries\n",
      "Loaded chunk_009.pkl, combined dictionary now has 4500 entries\n",
      "Loaded chunk_010.pkl, combined dictionary now has 5000 entries\n",
      "Loaded chunk_011.pkl, combined dictionary now has 5500 entries\n",
      "Loaded chunk_012.pkl, combined dictionary now has 6000 entries\n",
      "Loaded chunk_013.pkl, combined dictionary now has 6500 entries\n",
      "Loaded chunk_014.pkl, combined dictionary now has 7000 entries\n",
      "Loaded chunk_015.pkl, combined dictionary now has 7500 entries\n",
      "Loaded chunk_016.pkl, combined dictionary now has 8000 entries\n",
      "Loaded chunk_017.pkl, combined dictionary now has 8500 entries\n",
      "Loaded chunk_018.pkl, combined dictionary now has 9000 entries\n",
      "Loaded chunk_019.pkl, combined dictionary now has 9500 entries\n",
      "Loaded chunk_020.pkl, combined dictionary now has 10000 entries\n",
      "Loaded chunk_021.pkl, combined dictionary now has 10500 entries\n",
      "Loaded chunk_022.pkl, combined dictionary now has 11000 entries\n",
      "Loaded chunk_023.pkl, combined dictionary now has 11500 entries\n",
      "Loaded chunk_024.pkl, combined dictionary now has 12000 entries\n",
      "Loaded chunk_025.pkl, combined dictionary now has 12500 entries\n",
      "Loaded chunk_026.pkl, combined dictionary now has 13000 entries\n",
      "Loaded chunk_027.pkl, combined dictionary now has 13500 entries\n",
      "Loaded chunk_028.pkl, combined dictionary now has 13769 entries\n"
     ]
    }
   ],
   "source": [
    "lookup_dict = combine_pickle_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later I realized the issue with the UniProt accession 645345 and I want to fix this in the lookup table. (Doing it after because it is single issue and rather just modify what I collected and not have to run all the look-ups again. I embedded this step in the conditional so that in case authors address things later so that `645345` isn't there and more consistent `Q5T1J5` is, this code won't add an unnecessary item to `lookup_dict`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '645345' in lookup_dict:\n",
    "    del lookup_dict['645345']\n",
    "    lookup_dict['645345'] = 'CHCHD2P9' # additional special handling later will insure `645345` gets changed to `Q5T1J5` in resulting CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it doesn't take up too much space (list 234K), store the lookup table in pickled form for having in case need for reference later. Or for re-running downstream steps without needing to make the lookup table again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"look_up_dict_all_13769_Uniprot_ACCs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(lookup_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it didn't take up too much space, I stored the lookup table in pickled form for having in case need for reference later. Or fo re-running without needing to make again. So optional code below to handle those optons. (Otherwise, in the way the code is in the cell, the cell runs fine without doing anything in present form.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if need to read in the GIANT lookup dictonary\n",
    "'''\n",
    "import pickle\n",
    "with open(\"look_up_dict_all_13769_Uniprot_ACCs.pkl\", \"rb\") as f:\n",
    "        lookup_dict = pickle.load(f)\n",
    "len(list(lookup_dict.keys()))\n",
    "'''\n",
    "\n",
    "# if need to save the GIANT lookup dictionary again for some reason\n",
    "'''\n",
    "import pickle\n",
    "with open(\"look_up_dict_all_13769_Uniprot_ACCs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(lookup_dict, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use lookup table to fix and balance the two columns, `'Uniprot_ACCs'` & `'genenames'`\n",
    "\n",
    "This is a step towards making a better CSV that adheres close to the author-provided one but follows better data management practices to make it easier to use Pandas `explode()` the way I had been when I though the two columns were in-order matched and I had not realized the extent of special cases buried in the data.\n",
    "\n",
    "Also will do some accounting along the way so I can have more numbers to relate what was going on in original CSV. One aspect of this will be to assess how many special cases there are and make it easy to find them more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13769"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"look_up_dict_all_13769_Uniprot_ACCs.pkl\", \"rb\") as f:\n",
    "        lookup_dict = pickle.load(f)\n",
    "len(list(lookup_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using the `lookup_dict` go through `rd_df` dataframe row by row and \n",
    "# replace the column Uniprot_ACCs and genenames with in-order matched, balanced\n",
    "# versions and do accounting on them to track things along the way.\n",
    "# The tracking dataframe that will result will be separate from the main one \n",
    "# because I need that to be good to save and replace the author-provided `hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922.csv`\n",
    "fixed_df = rd_df.copy()\n",
    "tracker_results_df = rd_df.copy()\n",
    "class Tracker:\n",
    "    def __init__(self):\n",
    "        self.tracking_list = []\n",
    "\n",
    "def make_ordered_fix_and_collect_info_about_balance(row, tracker):\n",
    "    # for each row iterate on the identfiers in the column Uniprot_ACCs and \n",
    "    # account for the corresponding genename value in the genenames column. \n",
    "    # THIS WILL PRODUCED SAME NUMBER IN EACH COLUMN - so they are BALANCED in \n",
    "    # new version.\n",
    "    # Do some tracking for accounting, too:\n",
    "    # What is left? If anything is left then `genenames_had_unaccounted_for`\n",
    "    # Also note if matching originally. Or if even balanced originally. (Note\n",
    "    # that 'balanced originally' will take into account what I know about some \n",
    "    # Uniprot accensions corresponding to two genenames to produce the ones like \n",
    "    # `SLX1A; SLX1B` or `HSFY1; HSFY2` that have the semi-colon between the two \n",
    "    # related genemaes for the single Uniprot_ACC.\n",
    "    # Or if one of the semi-colons is involved, etc.\n",
    "    new_Uniprot_ACCs_list = []\n",
    "    new_genenames_list = []\n",
    "    original_ACCs_string = row['Uniprot_ACCs']\n",
    "    original_ACCs_as_list = row['Uniprot_ACCs'].split()\n",
    "    original_genenames_string = row['genenames']\n",
    "    original_genenames_as_list = row['genenames'].split()\n",
    "    # if original `genenames` has semi-colons, fix list based on what I had seen\n",
    "    # in analysis earlier in this notebook, so what looks like two separated by\n",
    "    # a `; ` are one without a space at all in the list. This is necessary to \n",
    "    # check the number of items balanced in each column later for \n",
    "    # 'balanced_originally' assessnent\n",
    "    if ';' in original_genenames_string:\n",
    "        # then fix the list\n",
    "        adjusted_original_genenames_as_list = []\n",
    "        i = 0\n",
    "        while i < len(original_genenames_as_list):\n",
    "            if original_genenames_as_list[i].endswith(';'):\n",
    "                new_string = original_genenames_as_list[i] + original_genenames_as_list[i+1]\n",
    "                adjusted_original_genenames_as_list.append(new_string)\n",
    "                i += 2  # Skip the next element because already merged it into one before it to make one genename text string corresponding to the ID for a specific single Uniprot_ACC; for checking for balance\n",
    "            else:\n",
    "                adjusted_original_genenames_as_list.append(original_genenames_as_list[i])\n",
    "                i += 1\n",
    "        original_genenames_as_list = adjusted_original_genenames_as_list # now set the list to the fixed one\n",
    "    what_remains = original_genenames_string # will be used to see if any genenames content not accounted for by content in Uniprot_ACCs column\n",
    "    matching_originally = False # set this and only change if established below\n",
    "    balanced_originally = False # set this and only change if established below\n",
    "    involves_semicolon = False # set this and only change if established below\n",
    "    SPECIAL_involved = False # set this and only change if established below\n",
    "    genenames_had_unaccounted_for = False # set this and only change if established below\n",
    "    what_genenames_had_unaccounted_for = \"NOTHING\" # set this and only change if established below\n",
    "\n",
    "    # Sort the original_ACCs_as_list based on the length of their corresponding genenames\n",
    "    # in lookup_dict (longest first)\n",
    "    sorted_original_ACCs = sorted(\n",
    "        original_ACCs_as_list,\n",
    "        key=lambda x: len(lookup_dict[x]),\n",
    "        reverse=True\n",
    "    )\n",
    "    # Now process in order of longest genename to shortest so for related genes like `TBL1X` and `TBL1XR1`, I don't end up removing `TBL1X` from `TBL1XR1` and just leave `R1` when making `what_remains`.\n",
    "    for original_ACC in sorted_original_ACCs:\n",
    "        if original_ACC == '645345': # handle this special case I noted where Uniprot_ACCs doesn't match a UniProt accessions; and this way if authors ever fix this won't happen and won't affect anything\n",
    "            new_Uniprot_ACCs_list.append('Q5T1J5')\n",
    "        else:\n",
    "            new_Uniprot_ACCs_list.append(original_ACC) # TYPICAL FOR ALL EXCEPT ONE!\n",
    "        matched_genename = lookup_dict[original_ACC]\n",
    "        new_genenames_list.append(matched_genename.replace(\" \",\"\")) # the replace will remove the space after the semi-colon for any that have that; maybe I could have done it when I made the lookup table but I had decided at the time to keep the option to adhere closer to the orginal author-provided CSV representation but now realize when 'exploding' the contents of the columns to have one identifier pairing per row, the space after the semi-colon will be an issue in the way I did it based on my earlier assessment of the author-provided CSV. On the plus side of leaving it in, when I remove the matching text from the text in the genenames column to see what may remain unaccounted for based on the IDs in the Uniprot_ACCs columns, I don't have to specially add back in the space to match the original form.\n",
    "        # While doing this iterating, start removing the accounted for \n",
    "        # identifiers from the second column string to end up with what is \n",
    "        # unaccounted for in the genenames column by the matching Uniprot_ACCs.\n",
    "        # Importantly, only replace the first occurrence of the exact genename; \n",
    "        # this and the sorting I did above will help with related genes like \n",
    "        # `TBL1X` and `TBL1XR1`.\n",
    "        parts = what_remains.split()\n",
    "        if lookup_dict[original_ACC] in parts:\n",
    "            parts.remove(lookup_dict[original_ACC]) #want the one that comes up\n",
    "            # from the lookup_dict matching so that ones that have semi-colon in \n",
    "            # them removes both of the two name ones at the same time; the one \n",
    "            # I put in `new_genenames_list` above had the space removed and so \n",
    "            # if used that one the semi-colon related ones won't get deleted \n",
    "            # from what_remains properly and end up being specified erroneously \n",
    "            # as remaining\n",
    "            what_remains = \" \".join(parts)\n",
    "        # need special handling for the cases where there is a semi-colon in the\n",
    "        # matched genename and note want the one that comes up from the \n",
    "        # lookup_dict matching so that ones that have semi-colon in them removes \n",
    "        # both of the two name ones at the same time; the one. I put in \n",
    "        # `new_genenames_list` above had the space removed and so if used that \n",
    "        # one the semi-colon related ones won't get deleted from what_remains \n",
    "        # properly and end up being specified erroneously as remaining.\n",
    "        if ';' in matched_genename and matched_genename.count(';') == 1: # some have more than one semi-colon and mostly I'll handle hardcoding below\n",
    "            # Handle the semicolon case by finding and removing the exact pair\n",
    "            parts = what_remains.split()\n",
    "            i = 0\n",
    "            while i < len(parts)-1:\n",
    "                if parts[i].endswith(';'):\n",
    "                    potential_pair = parts[i] + ' ' + parts[i+1]\n",
    "                    if potential_pair == matched_genename:\n",
    "                        # Remove both parts of the pair\n",
    "                        parts.pop(i)\n",
    "                        parts.pop(i)  # don't increment i since we removed two items\n",
    "                        break\n",
    "                    else:\n",
    "                        i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            what_remains = ' '.join(parts)\n",
    "        elif original_ACC in ['O15482','P23610','P62805','P68431','Q71DI3','P62807','P62807','P0C0S8']:\n",
    "            # handle some special cases where there's semi-colons so default below won't work and so few that it isn't worth working out programmatically\n",
    "            # SET UP TO Handle several special cases I noted, so proper text gets removed and don't add extra SPECIAL case to result CSV:\n",
    "            if original_ACC == 'O15482':\n",
    "                matched_genename = 'TEX28; TEX28P1; TEX28P2' # ones with more than a single semi-colon. Just going to hardcode handling removal so not tagged as unaccounted for.\n",
    "            if original_ACC == 'P23610':\n",
    "                matched_genename = 'F8A1; F8A2; F8A3'\n",
    "            if original_ACC == 'P62805':\n",
    "                matched_genename = 'H4C1; H4C2; H4C3; H4C4; H4C5; H4C6; H4C8; H4C9; H4C11; H4C12; H4C13; H4C14; H4C15; H4C16'\n",
    "            if original_ACC == 'P68431':\n",
    "                matched_genename = 'H3C1; H3C2; H3C3; H3C4; H3C6; H3C7; H3C8; H3C10; H3C11; H3C12'\n",
    "            if original_ACC == 'Q71DI3':\n",
    "                matched_genename = 'H3C15; H3C14; H3C13'\n",
    "            if original_ACC == 'P62807':\n",
    "                matched_genename = 'H2BC4; H2BC6; H2BC7; H2BC8; H2BC10'\n",
    "            if original_ACC == 'P0C0S8':\n",
    "                matched_genename = 'H2AC11; H2AC13; H2AC15; H2AC16; H2AC17'\n",
    "            # END SET UP FOR SEVERAL SPECIAL CASES WITH MANY SEMI-COLONS\n",
    "            what_remains = what_remains.replace(matched_genename,'')\n",
    "        else:\n",
    "            # Handle single genename case (when there is no semi-colon)\n",
    "            parts = what_remains.split()\n",
    "            # SET UP TO Handle several special cases I noted, so proper text gets removed and don't add extra SPECIAL case to resulting CSV:\n",
    "            if matched_genename == 'SLC66A1LP':\n",
    "                matched_genename = 'SLC66A1L' # makes it match what author provided file had for that one; they used one of its few synonyms\n",
    "            if matched_genename == 'PRP4K':\n",
    "                matched_genename = 'PRPF4B' # makes it match what author provided file had for that one; they used one of its few synonyms\n",
    "            if matched_genename == 'CFAP263':\n",
    "                matched_genename = 'CCDC113' # makes it match what author provided file had for that one; they used its synonym\n",
    "            if matched_genename == 'CFAP337':\n",
    "                matched_genename = 'WDR49' # makes it match what author provided file had for that one; they used its synonym\n",
    "            if matched_genename == 'FERRY3':\n",
    "                matched_genename = 'C12orf4' # makes it match what author provided file had for that one; they used its synonym\n",
    "            if matched_genename == 'CFAP184':\n",
    "                matched_genename = 'CCDC96' # makes it match what author provided file had for that one; they used its synonym\n",
    "            # END SET UP FOR SEVERAL SPECIAL CASES\n",
    "            if matched_genename in parts:\n",
    "                parts.remove(matched_genename)\n",
    "                what_remains = ' '.join(parts)\n",
    "\n",
    "    # now decide if anything extra has to be added to new_genenames_string by\n",
    "    # accounting for everything in original add adding what remains. Anything \n",
    "    # that gets added will need a 'SPECIAL_'-branded id placeholder added to the\n",
    "    # new_Uniprot_ACCs_list so that things remain balanced\n",
    "    if what_remains.strip():\n",
    "        genenames_had_unaccounted_for = True # set this for accounting\n",
    "        what_genenames_had_unaccounted_for = what_remains.strip() # set this for accounting\n",
    "        # now iterate on what splitting that string at the spaces returns and \n",
    "        # make a corresponding 'SPECIAL_'-labeled entry placeholder in the \n",
    "        unaccounted_genenames_list = what_genenames_had_unaccounted_for.split()\n",
    "        for unaccounted_gename in unaccounted_genenames_list:\n",
    "            new_Uniprot_ACCs_list.append('SPECIAL_unaccounted_gene') # NOTE, I used iterating on running this function & finding this occurence in the data to progressively add handling two types of 'special cases' (one type being those with threee or more semi-colons and the other being where the author-provided file use a synonyms) and ultimately end up with NO instances of `SPECIAL_unaccounted_gene` being necessary in my fixed in-order matched, balanced verison of the data.\n",
    "            new_genenames_list.append(unaccounted_gename)\n",
    "    assert (len(new_Uniprot_ACCs_list) == len(new_genenames_list)) #sanity \n",
    "    # check; they definitely should balance still\n",
    "    new_Uniprot_ACCs_string = \" \".join(new_Uniprot_ACCs_list)\n",
    "    new_genenames_string = \" \".join(new_genenames_list)\n",
    "    # Now that done making in-order matched & balanced content, assign the \n",
    "    # appropriate text for those two columns to be returned when row returned at \n",
    "    # end of this entire function (before getting to end of this function \n",
    "    # though, will do some accounting)\n",
    "    row['Uniprot_ACCs'] = new_Uniprot_ACCs_string\n",
    "    row['genenames'] = new_genenames_string\n",
    "\n",
    "    # Check original state for collecting some details\n",
    "    '''\n",
    "    for idx,oa in enumerate(original_ACCs_as_list):\n",
    "        if original_genenames_as_list[idx] != lookup_dict[oa]:\n",
    "            matching_originally = False\n",
    "            break\n",
    "\n",
    "    matching_originally = all(\n",
    "        original_genenames_as_list[i] == lookup_dict[oa]\n",
    "        for i, oa in enumerate(original_ACCs_as_list)\n",
    "    )\n",
    "    '''\n",
    "    if len(original_ACCs_as_list) == len(original_genenames_as_list):\n",
    "        balanced_originally = True\n",
    "        # Only worth checking matching if balanced because otherwise get out `IndexError: list index out of range` as try to check `original_genenames_as_list[i]`\n",
    "        matching_originally = all(\n",
    "            original_genenames_as_list[i] == lookup_dict[oa]\n",
    "            for i, oa in enumerate(original_ACCs_as_list)\n",
    "        )\n",
    "    if ';' in row['genenames']:\n",
    "        involves_semicolon = True\n",
    "    # Plus check the new state as part of the information being collected; the rows in the tracker will correspond to the fixed CSV to be made and so it will be pertinent to the corresponding row and I don't want to pollute the one to be fixed\n",
    "    if any('SPECIAL' in item for item in new_Uniprot_ACCs_list + new_genenames_list):\n",
    "        SPECIAL_involved =True\n",
    "    # want to track, 'matching_originally','balanced originally','involves_semicolon','SPECIAL_involved','genenames_had_unaccounted_for','what_genenames_had_unaccounted_for'\n",
    "    # This way later can use count of the numbers there to easy to see what is going on.\n",
    "    tracker.tracking_list.append(\n",
    "                            {\n",
    "                            'matching_originally':matching_originally, \n",
    "                            'balanced_originally':balanced_originally, \n",
    "                            'involves_semicolon': involves_semicolon,\n",
    "                            'SPECIAL_involved': SPECIAL_involved,\n",
    "                            'genenames_had_unaccounted_for': genenames_had_unaccounted_for,\n",
    "                            'what_genenames_had_unaccounted_for': what_genenames_had_unaccounted_for\n",
    "                            })  # Append to the class attribute list a dictionary\n",
    "    return row\n",
    "\n",
    "tracker = Tracker()\n",
    "fixed_df = fixed_df.apply(make_ordered_fix_and_collect_info_about_balance, args=(tracker,), axis=1)\n",
    "# Save this as CSV\n",
    "fixed_df.to_csv(\"hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922InOrderMatched.csv\",index = False)\n",
    "# will add to the tracker df as columns the lists corresponding to the values of the list of dictionaries\n",
    "#matching_originally_values = [d['matching_originally'] for d in tracker]\n",
    "#balanced_originally_values = [d['balanced_originally'] for d in tracker]\n",
    "#involves_semicolon_values = [d['involves_semicolon'] for d in tracker]\n",
    "#SPECIAL_involved_values = [d['SPECIAL_involved'] for d in tracker]\n",
    "#genenames_had_unaccounted_for_values = [d['genenames_had_unaccounted_for'] for d in tracker]\n",
    "#what_genenames_had_unaccounted_for_values = [d['what_genenames_had_unaccounted_for'] for d in tracker]\n",
    "#tracker_results_df['matching_originally'] = matching_originally_values\n",
    "tracker_results_df['matching_originally'] = [d['matching_originally'] for d in tracker.tracking_list] \n",
    "tracker_results_df['balanced_originally'] = [d['balanced_originally'] for d in tracker.tracking_list] \n",
    "tracker_results_df['involves_semicolon'] = [d['involves_semicolon'] for d in tracker.tracking_list]\n",
    "tracker_results_df['SPECIAL_involved'] = [d['involves_semicolon'] for d in tracker.tracking_list]\n",
    "tracker_results_df['genenames_had_unaccounted_for'] = [d['genenames_had_unaccounted_for'] for d in tracker.tracking_list]\n",
    "tracker_results_df['what_genenames_had_unaccounted_for'] = [d['what_genenames_had_unaccounted_for'] for d in tracker.tracking_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP3_ID</th>\n",
       "      <th>ComplexConfidence</th>\n",
       "      <th>Uniprot_ACCs</th>\n",
       "      <th>genenames</th>\n",
       "      <th>matching_originally</th>\n",
       "      <th>balanced_originally</th>\n",
       "      <th>involves_semicolon</th>\n",
       "      <th>SPECIAL_involved</th>\n",
       "      <th>genenames_had_unaccounted_for</th>\n",
       "      <th>what_genenames_had_unaccounted_for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huMAP3_00000.1</td>\n",
       "      <td>1</td>\n",
       "      <td>P20963 Q9NWV4 Q9UGQ2</td>\n",
       "      <td>CD247 CACFD1 CZIB</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NOTHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huMAP3_00001.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O94887 Q9NQ92 Q9NWB1</td>\n",
       "      <td>FARP2 COPRS RBFOX1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NOTHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huMAP3_00002.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q8N3D4 Q9Y3A4</td>\n",
       "      <td>RRP7A EHBP1L1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NOTHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huMAP3_00003.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O00429 Q5T2D2</td>\n",
       "      <td>DNM1L TREML2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NOTHING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huMAP3_00004.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O95460 P21941 P78540 Q9H267 Q9H9C1</td>\n",
       "      <td>MATN4 MATN1 ARG2 VPS33B VIPAS39</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NOTHING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        HuMAP3_ID  ComplexConfidence                        Uniprot_ACCs  \\\n",
       "0  huMAP3_00000.1                  1                P20963 Q9NWV4 Q9UGQ2   \n",
       "1  huMAP3_00001.1                  1                O94887 Q9NQ92 Q9NWB1   \n",
       "2  huMAP3_00002.1                  1                       Q8N3D4 Q9Y3A4   \n",
       "3  huMAP3_00003.1                  1                       O00429 Q5T2D2   \n",
       "4  huMAP3_00004.1                  1  O95460 P21941 P78540 Q9H267 Q9H9C1   \n",
       "\n",
       "                         genenames  matching_originally  balanced_originally  \\\n",
       "0                CD247 CACFD1 CZIB                False                 True   \n",
       "1               FARP2 COPRS RBFOX1                 True                 True   \n",
       "2                    RRP7A EHBP1L1                False                 True   \n",
       "3                     DNM1L TREML2                 True                 True   \n",
       "4  MATN4 MATN1 ARG2 VPS33B VIPAS39                 True                 True   \n",
       "\n",
       "   involves_semicolon  SPECIAL_involved  genenames_had_unaccounted_for  \\\n",
       "0               False             False                          False   \n",
       "1               False             False                          False   \n",
       "2               False             False                          False   \n",
       "3               False             False                          False   \n",
       "4               False             False                          False   \n",
       "\n",
       "  what_genenames_had_unaccounted_for  \n",
       "0                            NOTHING  \n",
       "1                            NOTHING  \n",
       "2                            NOTHING  \n",
       "3                            NOTHING  \n",
       "4                            NOTHING  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know from searching for 'SPECIAL_unaccounted' in the resulting CSV that my iterating on things and adding handling for special cases involving the use of synonym names and handling caseses with two or more semi-colons, i.e., three or more gene names for same UniProt accession, eliminated all those, so at this point `genenames_had_unaccounted_for` and\t`what_genenames_had_unaccounted_for` are moot and can be removed from the tracker data.  \n",
    "Let's verify that here and then remove those two tracking columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genenames_had_unaccounted_for</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>15326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   genenames_had_unaccounted_for  count\n",
       "0                          False  15326"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unf_count_df = tracker_results_df.copy().value_counts('genenames_had_unaccounted_for').reset_index()\n",
    "unf_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP3_ID</th>\n",
       "      <th>ComplexConfidence</th>\n",
       "      <th>Uniprot_ACCs</th>\n",
       "      <th>genenames</th>\n",
       "      <th>matching_originally</th>\n",
       "      <th>balanced_originally</th>\n",
       "      <th>involves_semicolon</th>\n",
       "      <th>SPECIAL_involved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huMAP3_00000.1</td>\n",
       "      <td>1</td>\n",
       "      <td>P20963 Q9NWV4 Q9UGQ2</td>\n",
       "      <td>CD247 CACFD1 CZIB</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huMAP3_00001.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O94887 Q9NQ92 Q9NWB1</td>\n",
       "      <td>FARP2 COPRS RBFOX1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huMAP3_00002.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q8N3D4 Q9Y3A4</td>\n",
       "      <td>RRP7A EHBP1L1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huMAP3_00003.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O00429 Q5T2D2</td>\n",
       "      <td>DNM1L TREML2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huMAP3_00004.1</td>\n",
       "      <td>1</td>\n",
       "      <td>O95460 P21941 P78540 Q9H267 Q9H9C1</td>\n",
       "      <td>MATN4 MATN1 ARG2 VPS33B VIPAS39</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        HuMAP3_ID  ComplexConfidence                        Uniprot_ACCs  \\\n",
       "0  huMAP3_00000.1                  1                P20963 Q9NWV4 Q9UGQ2   \n",
       "1  huMAP3_00001.1                  1                O94887 Q9NQ92 Q9NWB1   \n",
       "2  huMAP3_00002.1                  1                       Q8N3D4 Q9Y3A4   \n",
       "3  huMAP3_00003.1                  1                       O00429 Q5T2D2   \n",
       "4  huMAP3_00004.1                  1  O95460 P21941 P78540 Q9H267 Q9H9C1   \n",
       "\n",
       "                         genenames  matching_originally  balanced_originally  \\\n",
       "0                CD247 CACFD1 CZIB                False                 True   \n",
       "1               FARP2 COPRS RBFOX1                 True                 True   \n",
       "2                    RRP7A EHBP1L1                False                 True   \n",
       "3                     DNM1L TREML2                 True                 True   \n",
       "4  MATN4 MATN1 ARG2 VPS33B VIPAS39                 True                 True   \n",
       "\n",
       "   involves_semicolon  SPECIAL_involved  \n",
       "0               False             False  \n",
       "1               False             False  \n",
       "2               False             False  \n",
       "3               False             False  \n",
       "4               False             False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker_results_df = tracker_results_df.drop(columns=['genenames_had_unaccounted_for', 'what_genenames_had_unaccounted_for'])\n",
    "tracker_results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the 'tracker_results_df' that parallels the dataframe I used to make the resulting CSV to get some idea of numbers of what rows were originally in-order matchings & balanced, plus some details of the results I made to fix things to be closer to what will easiluy make tidy data using Pandas `explode()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching_originally</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>9679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>5647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matching_originally  count\n",
       "0                 True   9679\n",
       "1                False   5647"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo_count_df = tracker_results_df.copy().value_counts('matching_originally').reset_index()\n",
    "mo_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching_originally</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.631541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.368459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matching_originally  proportion\n",
       "0                 True    0.631541\n",
       "1                False    0.368459"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo_count_df = tracker_results_df.copy().value_counts('matching_originally', normalize=True).reset_index()\n",
    "mo_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So way more than half were matchcing the order in the two columns, `Uniprot_ACCs` & `genenames`, in the original CsV. In fact, about two-thirds matched, and so I wasn't totally clueless about not picking up on that earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_originally</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>15192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   balanced_originally  count\n",
       "0                 True  15192\n",
       "1                False    134"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bal_count_df = tracker_results_df.copy().value_counts('balanced_originally').reset_index()\n",
    "bal_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was intitially surprised that number is so low since I had originall noted 279 having different number of items in the same row, but that was before I accounted for the semi-colon with space use to specify multiple gene names for a single protein. I think that accounts for the discrepancy because here I was counting those as one and not just use space character to do the string splitting, like when I saw 279."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>involves_semicolon</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>15149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   involves_semicolon  count\n",
       "0               False  15149\n",
       "1                True    177"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_count_df = tracker_results_df.copy().value_counts('involves_semicolon').reset_index()\n",
    "sc_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So given the 134 not balanced originally, and what I just mentioned about those using semi-colons combined with a space to indicate related gene names for same UniProt accession identifier, the ones that incolve the semi-colon should somewhat account for the rest. Let's sum those up and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "177 + 134 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That number is higher than 279 because just by chance some of those would balance when split on spaces and so I would have missed some, but it is in the same ball-park.\n",
    "\n",
    "In my 'fixed' version of the CSV data what is the amount of 'SPECIAL' cases I identified?\n",
    "(Note there were no cases of 'not_known' in there because while putting these last steps together to do this accounting, I went back and looked at the issue with UniProt accession `645345` and found I knew it had a UniProt accession & gene name I could assign with some special handling. So I eliminted any of that type with some additional handling.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPECIAL_involved</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>15149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SPECIAL_involved  count\n",
       "0             False  15149\n",
       "1              True    177"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_count_df = tracker_results_df.copy().value_counts('SPECIAL_involved').reset_index()\n",
    "sp_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SPECIAL_involved</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.988451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>0.011549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SPECIAL_involved  proportion\n",
       "0             False    0.988451\n",
       "1              True    0.011549"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_count_df = tracker_results_df.copy().value_counts('SPECIAL_involved', normalize=True).reset_index()\n",
    "sp_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's a fairly small fraction , barely above 1%, that are 'special' cases in there now.\n",
    "And even if these are special there is matching information in both colunns in my 'fixed' CSV to keep things explodable and utimately tidy.\n",
    "\n",
    "**Use `hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922InOrderMatched.csv` saved above the 'accounting' I just did.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next few cells (stored converted to 'raw') for development debugging of lookup table generating only; leave as found unless you are Wayne troubleshooting:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# reload this for when testing in the next cell the main part of main lookup function\n",
    "import pickle\n",
    "with open(\"list_all_13769_uniue_Uniprot_ACCs.pkl\", \"rb\") as f:\n",
    "        ids_in_Uniprot_ACCs = pickle.load(f)\n",
    "len(ids_in_Uniprot_ACCs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# MAIN PART OF MAIN FUNCTION (`process_uniprot_chunk`) FOR LOOKUP COLLECTING- FOR DEBUGGING - for example `ids_in_Uniprot_ACCs[865:871]` had an issue & used this to diagnose\n",
    "# When actually running, will do in chunks.\n",
    "ids_in_Uniprot_ACCs = ids_in_Uniprot_ACCs[1001:1200] #Only uncommented during development!!\n",
    "#ids_in_Uniprot_ACCs = ['Q96LI6'] # FOR DEBUGGING, example of one with two genenames\n",
    "lookup_dict = {} #keys will be identifiers in the 'Uniprot_ACCs' column with the values to be the gene name in all but rare special cases where need special hadnlign\n",
    "from unipressed import UniprotkbClient\n",
    "import time\n",
    "import requests\n",
    "for uniprot_id in ids_in_Uniprot_ACCs:\n",
    "    #print(uniprot_id) # FOR DEBUGGING, uncomment for special cases, like `865:871` I want to know what ID is when there's an issue I need to track\n",
    "    try:\n",
    "        uniprot_record = UniprotkbClient.fetch_one(uniprot_id)\n",
    "        #lookup_dict[uniprot_id] = uniprot_record['genes'][0]['geneName']['value'] #worked for when one 'gene' in the list returned by `uniprot_record['genes']`\n",
    "        # HOWEVER...\n",
    "        # some IDs like `Q96LI6` and `Q9BQ83` give more than one gene for `uniprot_record['genes']` and the CSV from the hu.MAP3 people was combining \n",
    "        # those to things like `HSFY1; HSFY2` and `SLX1A; SLX1B`, respectively. To do accurate accounting and keep close to that, I want to recapitulate that, too.\n",
    "        if 'genes' in uniprot_record:\n",
    "            lookup_dict[uniprot_id] = '; '.join([x['geneName']['value'] for x in uniprot_record['genes']])\n",
    "        else:\n",
    "            if uniprot_id == 'B3KT37': # special case I looked into\n",
    "                lookup_dict[uniprot_id] = 'SPECIALin_UniProt_and_VETELKLIC_part_similar_to_YWHAE_but_no_official_gene'\n",
    "            else:\n",
    "                lookup_dict[uniprot_id] = 'SPECIALin_UniProt_but_no_gene'\n",
    "    except requests.exceptions.HTTPError as e:  # Use requests.exceptions.HTTPError\n",
    "        # Handle 400 Bad Request error (likely missing UniProt ID)\n",
    "        if e.response.status_code == 400:\n",
    "          lookup_dict[uniprot_id] = 'SPECIALnot_in_UniProt'\n",
    "          print(f\"UniProt ID '{uniprot_id}' not found. Marked as 'not_known' in lookup_dict.\")\n",
    "        else:\n",
    "          # Raise any other HTTP errors\n",
    "          raise e\n",
    "    #print(uniprot_record['genes'][0]['geneName']['value'])\n",
    "    time.sleep(1.12) # don't slam the API"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# final cell  for when testing in the next cell the main part of main lookup function\n",
    "list(lookup_dict.items())[:15]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# OLDER VERSION OF CODE THAT WORKED BUT DIDN'T ALLOW RESUMING EASILY This first cell sets things up so can run in chunks to progressively get to complete lookup dictionary eventually\n",
    "from unipressed import UniprotkbClient\n",
    "import time\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def process_uniprot_chunk(id_list):\n",
    "    \"\"\"\n",
    "    Process a chunk of UniProt IDs and return a dictionary of ID to gene name mappings.\n",
    "    \n",
    "    Args:\n",
    "        id_list (list): List of UniProt IDs to process\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with UniProt IDs as keys and gene names as values\n",
    "    \"\"\"\n",
    "    chunk_dict = {}\n",
    "    for uniprot_id in id_list:\n",
    "        try:\n",
    "            uniprot_record = UniprotkbClient.fetch_one(uniprot_id)\n",
    "            #lookup_dict[uniprot_id] = uniprot_record['genes'][0]['geneName']['value'] #worked for when one 'gene' in the list returned by `uniprot_record['genes']`\n",
    "            # HOWEVER...\n",
    "            # some IDs like `Q96LI6` and `Q9BQ83` give more than one gene for `uniprot_record['genes']` and the CSV from the hu.MAP3 people was combining \n",
    "            # those to things like `HSFY1; HSFY2` and `SLX1A; SLX1B`, respectively. To do accurate accounting and keep close to that, I want to recapitulate that, too.\n",
    "            if 'genes' in uniprot_record:\n",
    "                chunk_dict[uniprot_id] = '; '.join([x['geneName']['value'] for x in uniprot_record['genes']])\n",
    "            else:\n",
    "                if uniprot_id == 'B3KT37': # special case I looked into\n",
    "                    chunk_dict[uniprot_id] = 'SPECIALin_UniProt_and_VETELKLIC_part_similar_to_YWHAE_but_no_official_gene'\n",
    "                else:\n",
    "                    chunk_dict[uniprot_id] = 'SPECIALin_UniProt_but_no_gene'\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response.status_code == 400:\n",
    "                chunk_dict[uniprot_id] = 'SPECIALnot_in_UniProt'\n",
    "                print(f\"UniProt ID '{uniprot_id}' not found. Marked as 'not_known' in lookup_dict.\")\n",
    "            else:\n",
    "                raise e\n",
    "        time.sleep(1.12)  # don't slam the API\n",
    "    return chunk_dict\n",
    "\n",
    "def process_all_ids_in_chunks(id_list, chunk_size=1000, output_dir='chunk_pickles'):\n",
    "    \"\"\"\n",
    "    Process all IDs in chunks and save each chunk as a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        id_list (list): List of all UniProt IDs to process\n",
    "        chunk_size (int): Size of each chunk\n",
    "        output_dir (str): Directory to save pickle files\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    total_chunks = (len(id_list) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "    \n",
    "    for i in range(0, len(id_list), chunk_size):\n",
    "        chunk_num = i // chunk_size + 1\n",
    "        chunk = id_list[i:i + chunk_size]  # This will handle the final chunk automatically\n",
    "        \n",
    "        # Process chunk\n",
    "        print(f\"Processing chunk {chunk_num}/{total_chunks} ({len(chunk)} IDs)\")\n",
    "        chunk_result = process_uniprot_chunk(chunk)\n",
    "        \n",
    "        # Save chunk to pickle file\n",
    "        pickle_filename = os.path.join(output_dir, f'chunk_{chunk_num:03d}.pkl')\n",
    "        with open(pickle_filename, 'wb') as f:\n",
    "            pickle.dump(chunk_result, f)\n",
    "        \n",
    "        print(f\"Saved {len(chunk_result)} entries to {pickle_filename}\")\n",
    "\n",
    "def combine_pickle_chunks(pickle_dir='chunk_pickles'):\n",
    "    \"\"\"\n",
    "    Combine all pickle files in the directory into a single dictionary.\n",
    "    \n",
    "    Args:\n",
    "        pickle_dir (str): Directory containing the pickle files\n",
    "    \n",
    "    Returns:\n",
    "        dict: Combined dictionary of all chunks\n",
    "    \"\"\"\n",
    "    combined_dict = {}\n",
    "    pickle_files = sorted([f for f in os.listdir(pickle_dir) if f.endswith('.pkl')])\n",
    "    \n",
    "    for pickle_file in pickle_files:\n",
    "        with open(os.path.join(pickle_dir, pickle_file), 'rb') as f:\n",
    "            chunk_dict = pickle.load(f)\n",
    "            combined_dict.update(chunk_dict)\n",
    "        print(f\"Loaded {pickle_file}, combined dictionary now has {len(combined_dict)} entries\")\n",
    "    \n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m480\u001b[39m) \u001b[38;5;66;03m#60 seconds times 8 minutes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mexecuteSomething\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mexecuteSomething\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuteSomething\u001b[39m():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#code here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for development; to keep kernel active\n",
    "import time\n",
    "\n",
    "def executeSomething():\n",
    "    #code here\n",
    "    print ('.')\n",
    "    time.sleep(480) #60 seconds times 8 minutes\n",
    "\n",
    "while True:\n",
    "    executeSomething()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
