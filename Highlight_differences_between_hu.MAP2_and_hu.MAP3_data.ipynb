{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e629ba-45b1-48e7-b97c-8fb990064dfa",
   "metadata": {},
   "source": [
    "# Highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data\n",
    "\n",
    "This notebook is primarily an intermediate to introducing to the next notebook in this series, ['Using snakemake to highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data for multiple identifiers'](Using_snakemake_to_highlight_differences_between_hu.MAP2_and_hu.MAP3_data_for_multiple_indentifiers..ipynb). Here the steps to see the difference between complexes represented in hu.MAP 2.0 and hu.MAP 3.0 data is done with one example. You can change the hard-coded values here to investigate the differences for others. However, chances are you are interested in the differnces for several genes/proteins. In ['Using snakemake to highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data for multiple identifiers'](Using_snakemake_to_highlight_differences_between_hu.MAP2_and_hu.MAP3_data_for_multiple_indentifiers.ipynb), it will work through the steps to run a Snakemake workflow to process the identifiers for many proteins/genes to generate summary reports highlighting differences in human complexes reported in hu.MAP 2.0 vs. hu.MAP 3.0 data. And that is more likely where you want to invest your time. You may want to quickly san the notebook below though to see the types of reports you'll expect when you give the snakemake file your list of identifiers. And with more details here it may help you troubleshoot such snakemake-generated reports. \n",
    "\n",
    "This effort just is meant to highlight possible differences. There is some approximating done to determine possible corresponding complexes that inherently brings in some possible judgement calls. You'll need to explore the results from each for yourself to compare in depth. The hu.MAP 2.0 data can be explored in sessions launched from my [humap2-binder repo](https://github.com/fomightez/humap2-binder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d5493-8f48-4b60-97b2-812219359018",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## Step #1: Preparation\n",
    "\n",
    "The preparation parallels the notebooks ['Use of hu.MAP 3.0 data programmatically with Python, taking advantage of Jupyter features'](Working_with_hu.MAP3_data_with_Python_in_Jupyter_Basics.ipynb) and ['Use of hu.MAP 2.0 data programmatically with Python, taking advantage of Jupyter features'](Working_with_hu.MAP3_data_with_Python_in_Jupyter_Basics.ipynb), and so see them for more insight.\n",
    "\n",
    "Get the data for hu.MAP 2.0 and hu.MAP 3.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80a36fc-fbb5-4980-aea5-25872d5aeb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  502k  100  502k    0     0  1870k      0 --:--:-- --:--:-- --:--:-- 1875k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1243k  100 1243k    0     0  2426k      0 --:--:-- --:--:-- --:--:-- 2424k\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/humap2-binder/refs/heads/main/additional_nbs/standardizing_initial_data/humap2_complexes_20200809InOrderMatched.csv\n",
    "!curl -OL https://raw.githubusercontent.com/fomightez/humap3-binder/refs/heads/main/additional_nbs/standardizing_initial_data/hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922InOrderMatched.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992d046-e280-4ddd-b4a6-e18906df959c",
   "metadata": {},
   "source": [
    "Get an accessory script for adding information about the proteins in the complexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d08cff4-c9ad-4065-8246-38e58b71f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2893  100  2893    0     0  16392      0 --:--:-- --:--:-- --:--:-- 16344\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/structurework/refs/heads/master/humap3-utilities/make_lookup_table_for_extra_info4complexes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf301a-75c8-4faa-9c2e-028d7cb4bc55",
   "metadata": {},
   "source": [
    "##### Put the data on the complexes into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6f2f0f-2231-4d35-b5c7-08d89d5aed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1007  100  1007    0     0   3547      0 --:--:-- --:--:-- --:--:--  3558\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/structurework/refs/heads/master/humap3-utilities/complexes_rawCSV_to_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84223330-171f-4ab6-8a64-053c5492a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inline script metadata from `\u001b[36mcomplexes_rawCSV_to_df.py\u001b[39m`\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0mReading inline script metadata from `\u001b[36mcomplexes_rawCSV_to_df.py\u001b[39m`\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m"
     ]
    }
   ],
   "source": [
    "!uv run complexes_rawCSV_to_df.py humap2_complexes_20200809InOrderMatched.csv\n",
    "import pandas as pd\n",
    "rd2_df = pd.read_pickle('raw_complexes_pickled_df.pkl')\n",
    "!uv run complexes_rawCSV_to_df.py hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922InOrderMatched.csv\n",
    "rd3_df = pd.read_pickle('raw_complexes_pickled_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e55a6-7064-430a-bacd-c9fbb7d2f9fe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a5e9d-b2a9-4085-8fde-3774fc6b5bbf",
   "metadata": {},
   "source": [
    "## Analyze complexes in hu.MAP 2.0 vs. hu.MAP 3.0 for a protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73616bb2-04f8-44ca-b6dc-2220c229eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"ROGDI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a71f0-4543-4f4c-8f8e-5a91b2866a1b",
   "metadata": {},
   "source": [
    "Get complexes that protein is in in both sets of data, adding extra information like done in earlier notebooks.  \n",
    "Then use that to make a summary table for change from version 2.0 to version 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3a0227-c905-4572-97e4-613e396f0f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HuMAP2_ID  Confidence Uniprot_ACCs genenames\n",
      "6   HuMAP2_01834           3       Q9Y4E6      WDR7\n",
      "7   HuMAP2_03388           2       Q9Y485     DMXL1\n",
      "8   HuMAP2_03388           2       Q9GZN7     ROGDI\n",
      "9   HuMAP2_03388           2       Q8TDJ6     DMXL2\n",
      "10  HuMAP2_03388           2       Q9Y4E6      WDR7\n",
      " \n",
      "          HuMAP3_ID  ComplexConfidence Uniprot_ACCs genenames\n",
      "297  huMAP3_13872.1                  6       Q8NI08     NCOA7\n",
      "298  huMAP3_13872.1                  6       Q8TDJ6     DMXL2\n",
      "299  huMAP3_13872.1                  6       Q9GZN7     ROGDI\n",
      "300  huMAP3_13872.1                  6       Q9Y485     DMXL1\n",
      "301  huMAP3_13872.1                  6       Q9Y4E6      WDR7\n"
     ]
    }
   ],
   "source": [
    "# Next few cells will run the query collecting all complexes it occurs with and adding details for both datasets\n",
    "pattern = fr'\\b{search_term}\\b' # Create a regex pattern with word boundaries\n",
    "d2_rows_with_term_df = rd2_df[rd2_df['Uniprot_ACCs'].str.contains(pattern, case=False, regex=True) | rd2_df['genenames'].str.contains(pattern, case=False, regex=True)].copy()\n",
    "d3_rows_with_term_df = rd3_df[rd3_df['Uniprot_ACCs'].str.contains(pattern, case=False, regex=True) | rd3_df['genenames'].str.contains(pattern, case=False, regex=True)].copy()\n",
    "# make the dataframe have each row be a single protein\n",
    "# to prepare to use pandas `explode()` to do that, first make the content in be lists\n",
    "d2_rows_with_term_df['Uniprot_ACCs'] = d2_rows_with_term_df['Uniprot_ACCs'].str.split()\n",
    "d3_rows_with_term_df['Uniprot_ACCs'] = d3_rows_with_term_df['Uniprot_ACCs'].str.split()\n",
    "d2_rows_with_term_df['genenames'] = d2_rows_with_term_df['genenames'].str.split()\n",
    "d3_rows_with_term_df['genenames'] = d3_rows_with_term_df['genenames'].str.split()\n",
    "# Now use explode to create a new row for each element in both columns\n",
    "df2_expanded = d2_rows_with_term_df.explode(['Uniprot_ACCs', 'genenames']).copy()\n",
    "df3_expanded = d3_rows_with_term_df.explode(['Uniprot_ACCs', 'genenames']).copy()\n",
    "# Reset the index \n",
    "df2_expanded = df2_expanded.reset_index(drop=True)\n",
    "df3_expanded = df3_expanded.reset_index(drop=True)\n",
    "# Display the first few rows of the expanded dataframe\n",
    "print(df2_expanded.tail())\n",
    "print(\" \")\n",
    "print(df3_expanded.tail())\n",
    "# Next add extra information from UniProt for each protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb2d4bb-f6ae-47f2-b978-2dbc76c76a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell makes lookup table with the extra information; it takes a while to run & so is in a cell on its own to save time during development\n",
    "df_expanded = pd.concat([df2_expanded,df3_expanded])\n",
    "%run -i make_lookup_table_for_extra_info4complexes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2245de-2b7a-4b9b-97f0-6e8a15932644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use collected information to enhance the dataframes\n",
    "pn_dict = {k: v['protein_name'] for k, v in lookup_dict.items()}\n",
    "disease_dict = {k: v['disease'] for k, v in lookup_dict.items()}\n",
    "synonyms_dict = {k: v['synonyms'] for k, v in lookup_dict.items()}\n",
    "df3_expanded['synonyms'] = df3_expanded['Uniprot_ACCs'].map(synonyms_dict)\n",
    "df3_expanded['protein_name'] = df3_expanded['Uniprot_ACCs'].map(pn_dict)\n",
    "df3_expanded['disease'] = df3_expanded['Uniprot_ACCs'].map(disease_dict)\n",
    "conf_val2text_dict = {\n",
    "    1: 'Extremely High',\n",
    "    2: 'Very High',\n",
    "    3: 'High',\n",
    "    4: 'Moderate High',\n",
    "    5: 'Medium High',\n",
    "    6: 'Medium'\n",
    "}\n",
    "# Use vectorized mapping to convert confidence values to text\n",
    "df3_expanded['ComplexConfidence'] = df3_expanded['ComplexConfidence'].map(conf_val2text_dict)\n",
    "base_uniprot_url = 'https://www.uniprot.org/uniprotkb/'\n",
    "df3_expanded = df3_expanded.assign(Link=base_uniprot_url + df3_expanded['Uniprot_ACCs'])\n",
    "\n",
    "# do same for 2.0 data\n",
    "df2_expanded['synonyms'] = df2_expanded['Uniprot_ACCs'].map(synonyms_dict)\n",
    "df2_expanded['protein_name'] = df2_expanded['Uniprot_ACCs'].map(pn_dict)\n",
    "df2_expanded['disease'] = df2_expanded['Uniprot_ACCs'].map(disease_dict)\n",
    "conf_val2text_dict = {\n",
    "    1: 'Extremely High',\n",
    "    2: 'Very High',\n",
    "    3: 'High',\n",
    "    4: 'Moderate High',\n",
    "    5: 'Medium High',\n",
    "    6: 'Medium'\n",
    "}\n",
    "# Use vectorized mapping to convert confidence values to text\n",
    "df2_expanded['Confidence'] = df2_expanded['Confidence'].map(conf_val2text_dict)\n",
    "base_uniprot_url = 'https://www.uniprot.org/uniprotkb/'\n",
    "df2_expanded = df2_expanded.assign(Link=base_uniprot_url + df2_expanded['Uniprot_ACCs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b27756a-ec22-4787-a2d5-6585cd232810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondences (considering shared complex members) for version 2 vs. 3 with improvement, shared items, and coverage ratio:\n",
      "   HuMAP2_ID size_df2  rank      HuMAP3_ID  size_df3 improvement_in_v3 shared_items coverage_ratio\n",
      "HuMAP2_01834        5     1 huMAP3_12042.1        56                51            4           80.0\n",
      "HuMAP2_03388        4     2 huMAP3_10511.1        35                31            3           75.0\n",
      "HuMAP2_01148        2     3 huMAP3_13872.1        29                27            2          100.0\n",
      "                          4 huMAP3_09477.1        26                na           na             na\n",
      "                          5 huMAP3_09873.1        24                na           na             na\n",
      "                          6 huMAP3_08678.1        22                na           na             na\n",
      "                          7 huMAP3_09242.1        20                na           na             na\n",
      "                          8 huMAP3_07329.1        20                na           na             na\n",
      "                          9 huMAP3_08381.1        20                na           na             na\n",
      "                         10 huMAP3_07099.1        18                na           na             na\n",
      "                         11 huMAP3_01501.1        15                na           na             na\n",
      "                         12 huMAP3_01899.1        11                na           na             na\n",
      "                         13 huMAP3_03469.1         4                na           na             na\n",
      "                         14 huMAP3_05952.1         2                na           na             na\n"
     ]
    }
   ],
   "source": [
    "# Try combining, because ranked by size probably correct if the number of shared items doesn't get any better if you re-order. Then fall back to ranking by share members\n",
    "# if doesn't seem to work that the shared items never gets any better from ranked size comparisons.\n",
    "# Get the groups from both dataframes\n",
    "df2_groups = df2_expanded.groupby('HuMAP2_ID')['Uniprot_ACCs'].apply(list)\n",
    "df3_groups = df3_expanded.groupby('HuMAP3_ID')['Uniprot_ACCs'].apply(list)\n",
    "\n",
    "# Convert lists to sets\n",
    "source1_sets = {idx: set(ids) for idx, ids in df2_groups.items()}\n",
    "source2_sets = {idx: set(ids) for idx, ids in df3_groups.items()}\n",
    "\n",
    "# Sort groups by size\n",
    "source1_sorted = sorted(source1_sets.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "source2_sorted = sorted(source2_sets.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# Function to check shared items\n",
    "def get_shared_items(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "# Function for weighted similarity\n",
    "def weighted_similarity(set1, set2, weight_jaccard=0.7):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    jaccard = len(intersection) / len(union) if union else 0\n",
    "    max_possible_intersection = max(len(set1), len(set2))\n",
    "    normalized_intersection = len(intersection) / max_possible_intersection if max_possible_intersection != 0 else 0\n",
    "    \n",
    "    return weight_jaccard * jaccard + (1 - weight_jaccard) * normalized_intersection\n",
    "\n",
    "# Correspondence tracking\n",
    "correspondences = []\n",
    "used_source2_indices = set()\n",
    "\n",
    "# First pass: Try to match by ranking and shared items\n",
    "for i, (humap2_id, set1) in enumerate(source1_sorted):\n",
    "    # If we've reached the end of source2 sorted list, break\n",
    "    if i >= len(source2_sorted):\n",
    "        break\n",
    "    \n",
    "    humap3_id, set2 = source2_sorted[i]\n",
    "    \n",
    "    # Check shared items\n",
    "    shared = get_shared_items(set1, set2)\n",
    "    \n",
    "    if len(shared) > 1:  # need greater than 1 and not zero because always going to be greater than zero because every complex has to have query identifier\n",
    "        correspondences.append({\n",
    "            'HuMAP2_ID': humap2_id,\n",
    "            'HuMAP3_ID': humap3_id,\n",
    "            'HuMAP2_size': len(set1),\n",
    "            'HuMAP3_size': len(set2),\n",
    "            'shared_items': len(shared),\n",
    "            'shared_elements': shared,\n",
    "            'weighted_similarity': weighted_similarity(set1, set2)\n",
    "        })\n",
    "        used_source2_indices.add(i)\n",
    "\n",
    "# Second pass: Fallback method for unmatched groups\n",
    "# Create a list of unused source2 indices\n",
    "unused_source2_indices = [\n",
    "    j for j in range(len(source2_sorted)) \n",
    "    if j not in used_source2_indices\n",
    "]\n",
    "\n",
    "# Fallback matching\n",
    "for humap2_id, set1 in source1_sorted:\n",
    "    # Skip if this HuMAP2 ID has already been matched\n",
    "    if any(corr['HuMAP2_ID'] == humap2_id for corr in correspondences):\n",
    "        continue\n",
    "    \n",
    "    # Find best match among unused source2 groups\n",
    "    best_match = None\n",
    "    best_similarity = -1\n",
    "    best_unused_index = -1\n",
    "    \n",
    "    for j in unused_source2_indices:\n",
    "        humap3_id, set2 = source2_sorted[j]\n",
    "        similarity = weighted_similarity(set1, set2)\n",
    "        \n",
    "        if similarity > best_similarity:\n",
    "            best_match = (humap3_id, set2)\n",
    "            best_similarity = similarity\n",
    "            best_unused_index = j\n",
    "    \n",
    "    # Add the best match if found\n",
    "    if best_match:\n",
    "        humap3_id, set2 = best_match\n",
    "        shared = get_shared_items(set1, set2)\n",
    "        \n",
    "        correspondences.append({\n",
    "            'HuMAP2_ID': humap2_id,\n",
    "            'HuMAP3_ID': humap3_id,\n",
    "            'HuMAP2_size': len(set1),\n",
    "            'HuMAP3_size': len(set2),\n",
    "            'shared_items': len(shared),\n",
    "            'shared_elements': shared,\n",
    "            'weighted_similarity': best_similarity\n",
    "        })\n",
    "        \n",
    "        # Remove this index from unused indices\n",
    "        unused_source2_indices.remove(best_unused_index)\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "df_correspondences = pd.DataFrame(correspondences)\n",
    "\n",
    "# Sort by HuMAP2 ID size\n",
    "df_correspondences = df_correspondences.sort_values('HuMAP2_size', ascending=False)\n",
    "\n",
    "\n",
    "# Make summary table  with those correspondences\n",
    "# Create a dictionary to map HuMAP2_ID to its corresponding details\n",
    "correspondences_dict = {\n",
    "    corr['HuMAP2_ID']: {\n",
    "        'HuMAP3_ID': corr['HuMAP3_ID'],\n",
    "        'size_df3': corr['HuMAP3_size'],\n",
    "        'shared_items': corr['shared_items'],\n",
    "        'weighted_similarity': corr['weighted_similarity']\n",
    "    } for corr in correspondences\n",
    "}\n",
    "\n",
    "# Modify the correspondence creation to use this new matching\n",
    "# First, create the base dataframe as before\n",
    "df2_sorted = df2_expanded.groupby('HuMAP2_ID').size().sort_values(ascending=False)\n",
    "df3_sorted = df3_expanded.groupby('HuMAP3_ID').size().sort_values(ascending=False)\n",
    "\n",
    "# Convert the series to dataframes\n",
    "df2_ranks = df2_sorted.reset_index()\n",
    "df3_ranks = df3_sorted.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df2_ranks.columns = ['HuMAP2_ID', 'size_df2']\n",
    "df3_ranks.columns = ['HuMAP3_ID', 'size_df3']\n",
    "\n",
    "# Add rank columns\n",
    "df2_ranks['rank'] = range(1, len(df2_ranks) + 1)\n",
    "df3_ranks['rank'] = range(1, len(df3_ranks) + 1)\n",
    "\n",
    "# Merge the rankings side by side\n",
    "correspondence = pd.merge(\n",
    "    df2_ranks,\n",
    "    df3_ranks,\n",
    "    on='rank',\n",
    "    how='outer'\n",
    ")\n",
    "# Add details from the correspondences\n",
    "def get_correspondence_details(row):\n",
    "    if pd.notna(row['HuMAP2_ID']) and row['HuMAP2_ID'] in correspondences_dict:\n",
    "        details = correspondences_dict[row['HuMAP2_ID']]\n",
    "        return pd.Series({\n",
    "            #'HuMAP3_ID': details['HuMAP3_ID'],\n",
    "            #'size_df3': details['size_df3'],\n",
    "            'shared_items': details['shared_items'],\n",
    "            'improvement_in_v3': int(details['size_df3'] - row['size_df2'])\n",
    "        })\n",
    "    return pd.Series({\n",
    "        #'HuMAP3_ID': '',\n",
    "        #'size_df3': 0,\n",
    "        'shared_items': 'na',\n",
    "        'improvement_in_v3': 'na'\n",
    "    })\n",
    "\n",
    "correspondence_details = correspondence.apply(get_correspondence_details, axis=1)\n",
    "correspondence = pd.concat([correspondence, correspondence_details], axis=1)\n",
    "# Calculate coverage ratio\n",
    "def cov_ratio(row):\n",
    "    if isinstance(row['shared_items'], (int, float)) and row['size_df2'] > 0 and row['size_df3'] > 0:\n",
    "        return round(row['shared_items'] / min(row['size_df2'], row['size_df3']) * 100, 2) \n",
    "    else:\n",
    "        return 'na'\n",
    "\n",
    "correspondence['coverage_ratio'] = correspondence.apply(cov_ratio, axis=1)\n",
    "\n",
    "# Prepare display\n",
    "correspondence = correspondence.sort_values('rank')\n",
    "correspondence['size_df2'] = correspondence['size_df2'].fillna(0).astype(int)\n",
    "correspondence['size_df3'] = correspondence['size_df3'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# Create display dataframe\n",
    "display_df = correspondence.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "\n",
    "# Update print statement\n",
    "print(\"\\nCorrespondences (considering shared complex members) for version 2 vs. 3 with improvement, shared items, and coverage ratio:\")\n",
    "print(display_df[['HuMAP2_ID', 'size_df2', 'rank', 'HuMAP3_ID', 'size_df3', 'improvement_in_v3', 'shared_items', 'coverage_ratio']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874e9c1-3330-4fdd-9a73-5886a06e2d91",
   "metadata": {},
   "source": [
    "That above should be a summary table quickly giving a sense of the improvement in 3.0 data. (Or lack thereof, if yo have substituted with your protein of interest?)\n",
    "\n",
    "Despite general improvment, I noticed some information present in version 2.0 complexes went missing in complexes reported in version 3.0.\n",
    "    \n",
    "#### Check for disappearing proteins in complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4886c09-efbe-4c9e-b0fe-fa34d1c0d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disappearing Items between Hu.MAP2.0 and Hu.MAP3.0 data:\n",
      "\n",
      "HuMAP2 ID: HuMAP2_01834 -> HuMAP3 ID: huMAP3_12042.1\n",
      "Total items in HuMAP2: 5\n",
      "Total items in HuMAP3: 56\n",
      "Percent of items lost: 20.00%\n",
      "UniProt ACCs Lost From Corresponding Complex:\n",
      "  - P50993 (AND NOT IN ANY OTHER Hu.MAP3.0 ROGDI-complexes!!!)\n",
      "\n",
      "HuMAP2 ID: HuMAP2_03388 -> HuMAP3 ID: huMAP3_10511.1\n",
      "Total items in HuMAP2: 4\n",
      "Total items in HuMAP3: 35\n",
      "Percent of items lost: 25.00%\n",
      "UniProt ACCs Lost From Corresponding Complex:\n",
      "  - Q8TDJ6 (But present in other, ROGDI-related Hu.MAP3.0 complexes)\n"
     ]
    }
   ],
   "source": [
    "# Make sure nothing present in HuMAP2 data disappeared in huMAP3 data\n",
    "# Function to compare UniProt ACCs between HuMAP2 and HuMAP3\n",
    "def check_disappearing_items(df_correspondences, df2_expanded, df3_expanded):\n",
    "    # Tracking disappearing items\n",
    "    disappearing_items = []\n",
    "    \n",
    "    for _, correspondence in df_correspondences.iterrows():\n",
    "        humap2_id = correspondence['HuMAP2_ID']\n",
    "        humap3_id = correspondence['HuMAP3_ID']\n",
    "        \n",
    "        # Get UniProt ACCs for this HuMAP2 group\n",
    "        humap2_accs = set(df2_expanded[df2_expanded['HuMAP2_ID'] == humap2_id]['Uniprot_ACCs'])\n",
    "        \n",
    "        # Get UniProt ACCs for this HuMAP3 group\n",
    "        humap3_accs = set(df3_expanded[df3_expanded['HuMAP3_ID'] == humap3_id]['Uniprot_ACCs'])\n",
    "        \n",
    "        # Find items in HuMAP2 that are not in HuMAP3\n",
    "        lost_items = humap2_accs - humap3_accs\n",
    "        \n",
    "        if lost_items:\n",
    "            disappearing_items.append({\n",
    "                'HuMAP2_ID': humap2_id,\n",
    "                'HuMAP3_ID': humap3_id,\n",
    "                'lost_items': lost_items,\n",
    "                'total_humap2_items': len(humap2_accs),\n",
    "                'total_humap3_items': len(humap3_accs),\n",
    "                'percent_lost': len(lost_items) / len(humap2_accs) * 100 if humap2_accs else 0\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame for easy analysis\n",
    "    df_disappearing = pd.DataFrame(disappearing_items)\n",
    "    \n",
    "    # Display results\n",
    "    global something_dropped_completely\n",
    "    something_dropped_completely = False\n",
    "    if not df_disappearing.empty:\n",
    "        print(\"\\nDisappearing Items between Hu.MAP2.0 and Hu.MAP3.0 data:\")\n",
    "        for _, row in df_disappearing.iterrows():\n",
    "            print(f\"\\nHuMAP2 ID: {row['HuMAP2_ID']} -> HuMAP3 ID: {row['HuMAP3_ID']}\")\n",
    "            print(f\"Total items in HuMAP2: {row['total_humap2_items']}\")\n",
    "            print(f\"Total items in HuMAP3: {row['total_humap3_items']}\")\n",
    "            print(f\"Percent of items lost: {row['percent_lost']:.2f}%\")\n",
    "            print(\"UniProt ACCs Lost From Corresponding Complex:\")\n",
    "            for item in row['lost_items']:\n",
    "                dropped_completely = (item not in df3_expanded['Uniprot_ACCs'].to_list())\n",
    "                if dropped_completely:\n",
    "                    print(f\"  - {item} (AND NOT IN ANY OTHER Hu.MAP3.0 {search_term}-complexes!!!)\")\n",
    "                    something_dropped_completely = True\n",
    "                else:\n",
    "                    print(f\"  - {item} (But present in other, {search_term}-related Hu.MAP3.0 complexes)\")\n",
    "        \n",
    "        # Optional: Save to CSV\n",
    "        df_disappearing.to_csv('disappearing_items.csv', index=False)\n",
    "        \n",
    "        return df_disappearing, something_dropped_completely\n",
    "    else:\n",
    "        print(\"\\nNo disappearing items found between HuMAP2 and HuMAP3.\")\n",
    "        return None, something_dropped_completely\n",
    "\n",
    "# Call the function\n",
    "disappearing_df, something_dropped_completely = check_disappearing_items(df_correspondences, df2_expanded, df3_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924889b2-8e87-44a8-aee0-0b4e0605ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hu.MAP2.0-Majority Complex Members Disappearing in Hu.MAP3.0 data:\n",
      "NONE\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Also make sure nothing that is in the majority of complexes in hu.MAP 2.0 data disappears entirely from hu.MAP 3.0 data.\n",
    "# That can only occur as the case if `something_dropped_completely`, that we've already check on, is True\n",
    "majority_item_dropped_completely = False #set up by declaring false\n",
    "if something_dropped_completely:\n",
    "    # collect information on what identifiers occur in the majority of complexes in hu.MAP 2.0 data\n",
    "    # the search term should always be present and can thus serve as a sanity check for this collection\n",
    "    majority_identifiers = []\n",
    "    def find_majority_uniprot_accs(df):\n",
    "        # Group by HuMAP2_ID and get unique Uniprot_ACCs for each complex\n",
    "        complex_accs = df.groupby('HuMAP2_ID')['Uniprot_ACCs'].unique()\n",
    "        \n",
    "        # Count total number of unique complexes\n",
    "        total_complexes = len(complex_accs)\n",
    "        \n",
    "        # Flatten and count occurrences of each Uniprot ACC across complexes\n",
    "        all_accs = df['Uniprot_ACCs'].tolist()\n",
    "        acc_counts = pd.Series(all_accs).value_counts()\n",
    "        \n",
    "        # Find ACCs that appear in more than 50% of complexes\n",
    "        majority_accs = acc_counts[acc_counts >= 0.51 * total_complexes].index.tolist()\n",
    "        \n",
    "        return majority_accs\n",
    "    if find_majority_uniprot_accs(df2_expanded):\n",
    "        majority_identifiers = find_majority_uniprot_accs(df2_expanded)\n",
    "    # Now check none of those `majority_identifiers` match the disappearing ones dropped completely\n",
    "    print(\"\\nHu.MAP2.0-Majority Complex Members Disappearing in Hu.MAP3.0 data:\")\n",
    "    for item in list(set.union(*disappearing_df['lost_items'])): # note `list(set.union(*disappearing_df['lost_items']))` gets the unique set members from the 'lost_items' column\n",
    "        if item in majority_identifiers:\n",
    "            majority_item_dropped_completely = (item not in df3_expanded['Uniprot_ACCs'].to_list())\n",
    "            if majority_item_dropped_completely:\n",
    "                print(f\"  - {item} IS IN MAJORITY OF Hu.MAP2.0 COMPLEXES YET NOT IN ANY Hu.MAP3.0 {search_term}-complexes!!! (PERHAPS CONCERNING?)\")\n",
    "                majority_item_dropped_completely = True\n",
    "    if majority_item_dropped_completely == False:\n",
    "        print(\"NONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3e9d1-4740-4c91-ae20-fc9df5ba3ce8",
   "metadata": {},
   "source": [
    " You can change the hard-coded values here to investigate the differences for others. However, chances are you are interested in the differnces for several genes/proteins, and so you'll want to first check out the next notebook in this series, ['Using snakemake to highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data for multiple identifiers'](Using_snakemake_to_highlight_differences_between_hu.MAP2_and_hu.MAP3_data_for_multiple_indentifiers.ipynb). That notebook will work through the steps to run a Snakemake workflow to process the identifiers for many proteins/genes to generate summary reports highlighting differences in human complexes reported in hu.MAP 2.0 vs. hu.MAP 3.0 data.\n",
    " \n",
    "-----\n",
    "\n",
    "Enjoy!\n",
    "\n",
    "See my [humap3-binder repo](https://github.com/fomightez/humap3-binder) and [humap3-utilities](https://github.com/fomightez/structurework/humap3-utilities) for related information & resources for this notebook.\n",
    "\n",
    "\n",
    "\n",
    "-----\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
