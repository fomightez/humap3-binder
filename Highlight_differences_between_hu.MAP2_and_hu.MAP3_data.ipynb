{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e629ba-45b1-48e7-b97c-8fb990064dfa",
   "metadata": {},
   "source": [
    "# Highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data\n",
    "\n",
    "This notebook is primarily an intermediate to introducing to the next notebook in this series, ['Using snakemake to highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data for multiple identifiers'](Using_snakemake_to_highlight_differences_between_hu.MAP2_and_hu.MAP3_data_for_multiple_indentifiers..ipynb). Here the steps to see the difference between complexes represented in hu.MAP 2.0 and hu.MAP 3.0 data is done with one example. You can change the hard-coded values here to investigate the differences for others. However, chances are you are interested in the differnces for several genes/proteins. In ['Using snakemake to highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data for multiple identifiers'](Using_snakemake_to_highlight_differences_between_hu.MAP2_and_hu.MAP3_data_for_multiple_indentifiers.ipynb), it will work through the steps to run a Snakemake workflow to process the identifiers for many proteins/genes to generate summary reports highlighting differences in human complexes reported in hu.MAP 2.0 vs. hu.MAP 3.0 data. And that is more likely where you want to invest your time. You may want to quickly san the notebook below though to see the types of reports you'll expect when you give the snakemake file your list of identifiers. And with more details here it may help you troubleshoot such snakemake-generated reports. \n",
    "\n",
    "This effort just is meant to highlight possible differences. There is some approximating done to determine possible corresponding complexes that inherently brings in some possible judgement calls. You'll need to explore the results from each for yourself to compare in depth. The hu.MAP 2.0 data can be explored in sessions launched from my [humap2-binder repo](https://github.com/fomightez/humap2-binder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d5493-8f48-4b60-97b2-812219359018",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## Step #1: Preparation\n",
    "\n",
    "The preparation parallels the notebooks ['Use of hu.MAP 3.0 data programmatically with Python, taking advantage of Jupyter features'](Working_with_hu.MAP3_data_with_Python_in_Jupyter_Basics.ipynb) and ['Use of hu.MAP 2.0 data programmatically with Python, taking advantage of Jupyter features'](Working_with_hu.MAP3_data_with_Python_in_Jupyter_Basics.ipynb), and so see them for more insight.\n",
    "\n",
    "Get the data for hu.MAP 2.0 and hu.MAP 3.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80a36fc-fbb5-4980-aea5-25872d5aeb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  502k  100  502k    0     0   990k      0 --:--:-- --:--:-- --:--:--  991k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1243k  100 1243k    0     0  2090k      0 --:--:-- --:--:-- --:--:-- 2090k\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/humap2-binder/refs/heads/main/additional_nbs/standardizing_initial_data/humap2_complexes_20200809InOrderMatched.csv\n",
    "!curl -OL https://raw.githubusercontent.com/fomightez/humap3-binder/refs/heads/main/additional_nbs/standardizing_initial_data/hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922InOrderMatched.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992d046-e280-4ddd-b4a6-e18906df959c",
   "metadata": {},
   "source": [
    "Get an accessory script for adding information about the proteins in the complexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d08cff4-c9ad-4065-8246-38e58b71f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2893  100  2893    0     0   6999      0 --:--:-- --:--:-- --:--:--  6987\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/structurework/refs/heads/master/humap3-utilities/make_lookup_table_for_extra_info4complexes.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf301a-75c8-4faa-9c2e-028d7cb4bc55",
   "metadata": {},
   "source": [
    "##### Put the data on the complexes into Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6f2f0f-2231-4d35-b5c7-08d89d5aed2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1007  100  1007    0     0   3210      0 --:--:-- --:--:-- --:--:--  3217\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/fomightez/structurework/refs/heads/master/humap3-utilities/complexes_rawCSV_to_df.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84223330-171f-4ab6-8a64-053c5492a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inline script metadata from `\u001b[36mcomplexes_rawCSV_to_df.py\u001b[39m`\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 163ms\u001b[0m\u001b[0m                              \u001b[0m         \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP2_ID</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>Uniprot_ACCs</th>\n",
       "      <th>genenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HuMAP2_00000</td>\n",
       "      <td>3</td>\n",
       "      <td>O95900 Q9BQS8</td>\n",
       "      <td>TRUB2 FYCO1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HuMAP2_00001</td>\n",
       "      <td>4</td>\n",
       "      <td>Q15102 P68402 Q15797 P08133 Q99426 Q9H4M9</td>\n",
       "      <td>PAFAH1B3 PAFAH1B2 SMAD1 ANXA6 TBCB EHD1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HuMAP2_00002</td>\n",
       "      <td>5</td>\n",
       "      <td>Q9UF11 A1KXE4 Q6ZRY4 Q9Y6M7 Q15038 O43251 Q930...</td>\n",
       "      <td>PLEKHB1 FAM168B RBPMS2 SLC4A7 DAZAP2 RBFOX2 RB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HuMAP2_00003</td>\n",
       "      <td>5</td>\n",
       "      <td>O14974 Q8WUM9 Q9Y5Y0 Q16563 Q14919 Q15836 Q299...</td>\n",
       "      <td>PPP1R12A SLC20A1 FLVCR1 SYPL1 DRAP1 VAMP3 MICA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HuMAP2_00004</td>\n",
       "      <td>4</td>\n",
       "      <td>Q8WV99 Q49A92 Q9NQT8 Q9H672 P20774</td>\n",
       "      <td>ZFAND2B C8orf34 KIF13B ASB7 OGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6960</th>\n",
       "      <td>HuMAP2_07014</td>\n",
       "      <td>4</td>\n",
       "      <td>Q9HC97 P31152 Q6S8J3 P13727 Q92871</td>\n",
       "      <td>GPR35 MAPK4 POTEE PRG2 PMM1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6961</th>\n",
       "      <td>HuMAP2_07015</td>\n",
       "      <td>4</td>\n",
       "      <td>Q96E29 Q8N5N7 Q96I51 Q9H5L6 O75127 Q9NPE2</td>\n",
       "      <td>MTERF3 MRPL50 RCC1L THAP9 PTCD1 NGRN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6962</th>\n",
       "      <td>HuMAP2_07016</td>\n",
       "      <td>5</td>\n",
       "      <td>O75319 Q96HN2 Q8NE31 O43865 P52952 Q2T9J0 Q9UP...</td>\n",
       "      <td>DUSP11 AHCYL2 FAM13C AHCYL1 NKX2-5 TYSND1 PDZR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6963</th>\n",
       "      <td>HuMAP2_07017</td>\n",
       "      <td>2</td>\n",
       "      <td>Q96GP6 Q53GT1 P49448</td>\n",
       "      <td>SCARF2 KLHL22 GLUD2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>HuMAP2_07018</td>\n",
       "      <td>2</td>\n",
       "      <td>Q2NKX9 Q86US8 Q9Y223</td>\n",
       "      <td>C2orf68 SMG6 GNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6965 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HuMAP2_ID  Confidence  \\\n",
       "0     HuMAP2_00000           3   \n",
       "1     HuMAP2_00001           4   \n",
       "2     HuMAP2_00002           5   \n",
       "3     HuMAP2_00003           5   \n",
       "4     HuMAP2_00004           4   \n",
       "...            ...         ...   \n",
       "6960  HuMAP2_07014           4   \n",
       "6961  HuMAP2_07015           4   \n",
       "6962  HuMAP2_07016           5   \n",
       "6963  HuMAP2_07017           2   \n",
       "6964  HuMAP2_07018           2   \n",
       "\n",
       "                                           Uniprot_ACCs  \\\n",
       "0                                         O95900 Q9BQS8   \n",
       "1             Q15102 P68402 Q15797 P08133 Q99426 Q9H4M9   \n",
       "2     Q9UF11 A1KXE4 Q6ZRY4 Q9Y6M7 Q15038 O43251 Q930...   \n",
       "3     O14974 Q8WUM9 Q9Y5Y0 Q16563 Q14919 Q15836 Q299...   \n",
       "4                    Q8WV99 Q49A92 Q9NQT8 Q9H672 P20774   \n",
       "...                                                 ...   \n",
       "6960                 Q9HC97 P31152 Q6S8J3 P13727 Q92871   \n",
       "6961          Q96E29 Q8N5N7 Q96I51 Q9H5L6 O75127 Q9NPE2   \n",
       "6962  O75319 Q96HN2 Q8NE31 O43865 P52952 Q2T9J0 Q9UP...   \n",
       "6963                               Q96GP6 Q53GT1 P49448   \n",
       "6964                               Q2NKX9 Q86US8 Q9Y223   \n",
       "\n",
       "                                              genenames  \n",
       "0                                           TRUB2 FYCO1  \n",
       "1               PAFAH1B3 PAFAH1B2 SMAD1 ANXA6 TBCB EHD1  \n",
       "2     PLEKHB1 FAM168B RBPMS2 SLC4A7 DAZAP2 RBFOX2 RB...  \n",
       "3     PPP1R12A SLC20A1 FLVCR1 SYPL1 DRAP1 VAMP3 MICA...  \n",
       "4                       ZFAND2B C8orf34 KIF13B ASB7 OGN  \n",
       "...                                                 ...  \n",
       "6960                        GPR35 MAPK4 POTEE PRG2 PMM1  \n",
       "6961               MTERF3 MRPL50 RCC1L THAP9 PTCD1 NGRN  \n",
       "6962  DUSP11 AHCYL2 FAM13C AHCYL1 NKX2-5 TYSND1 PDZR...  \n",
       "6963                                SCARF2 KLHL22 GLUD2  \n",
       "6964                                   C2orf68 SMG6 GNE  \n",
       "\n",
       "[6965 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!uv run complexes_rawCSV_to_df.py humap2_complexes_20200809InOrderMatched.csv\n",
    "import pandas as pd\n",
    "rd2_df = pd.read_pickle('raw_complexes_pickled_df.pkl')\n",
    "rd2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf15ae5-dcc7-4ca2-91a0-e1087c915791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading inline script metadata from `\u001b[36mcomplexes_rawCSV_to_df.py\u001b[39m`\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP3_ID</th>\n",
       "      <th>ComplexConfidence</th>\n",
       "      <th>Uniprot_ACCs</th>\n",
       "      <th>genenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huMAP3_00000.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q9UGQ2 P20963 Q9NWV4</td>\n",
       "      <td>CACFD1 CD247 CZIB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huMAP3_00001.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q9NWB1 O94887 Q9NQ92</td>\n",
       "      <td>RBFOX1 FARP2 COPRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huMAP3_00002.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q8N3D4 Q9Y3A4</td>\n",
       "      <td>EHBP1L1 RRP7A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huMAP3_00003.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q5T2D2 O00429</td>\n",
       "      <td>TREML2 DNM1L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>huMAP3_00004.1</td>\n",
       "      <td>1</td>\n",
       "      <td>Q9H9C1 Q9H267 O95460 P21941 P78540</td>\n",
       "      <td>VIPAS39 VPS33B MATN4 MATN1 ARG2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15321</th>\n",
       "      <td>huMAP3_15345.1</td>\n",
       "      <td>6</td>\n",
       "      <td>O14628 Q3SXZ3</td>\n",
       "      <td>ZNF195 ZNF718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15322</th>\n",
       "      <td>huMAP3_15346.1</td>\n",
       "      <td>6</td>\n",
       "      <td>Q6ZWT7 P08910 Q86VD1 Q9UJQ1 Q9Y6X9</td>\n",
       "      <td>MBOAT2 ABHD2 MORC1 LAMP5 MORC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15323</th>\n",
       "      <td>huMAP3_15347.1</td>\n",
       "      <td>6</td>\n",
       "      <td>A6ND91 Q4V339</td>\n",
       "      <td>ASPDH ZNG1F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15324</th>\n",
       "      <td>huMAP3_15348.1</td>\n",
       "      <td>6</td>\n",
       "      <td>A6NKF2 P08217 Q8IVW6 Q99856</td>\n",
       "      <td>ARID3C CELA2A ARID3B ARID3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15325</th>\n",
       "      <td>huMAP3_15349.1</td>\n",
       "      <td>6</td>\n",
       "      <td>Q5SRH9 P51888 Q9NRD9 Q9UHX1</td>\n",
       "      <td>TTC39A PRELP DUOX1 PUF60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15326 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            HuMAP3_ID  ComplexConfidence                        Uniprot_ACCs  \\\n",
       "0      huMAP3_00000.1                  1                Q9UGQ2 P20963 Q9NWV4   \n",
       "1      huMAP3_00001.1                  1                Q9NWB1 O94887 Q9NQ92   \n",
       "2      huMAP3_00002.1                  1                       Q8N3D4 Q9Y3A4   \n",
       "3      huMAP3_00003.1                  1                       Q5T2D2 O00429   \n",
       "4      huMAP3_00004.1                  1  Q9H9C1 Q9H267 O95460 P21941 P78540   \n",
       "...               ...                ...                                 ...   \n",
       "15321  huMAP3_15345.1                  6                       O14628 Q3SXZ3   \n",
       "15322  huMAP3_15346.1                  6  Q6ZWT7 P08910 Q86VD1 Q9UJQ1 Q9Y6X9   \n",
       "15323  huMAP3_15347.1                  6                       A6ND91 Q4V339   \n",
       "15324  huMAP3_15348.1                  6         A6NKF2 P08217 Q8IVW6 Q99856   \n",
       "15325  huMAP3_15349.1                  6         Q5SRH9 P51888 Q9NRD9 Q9UHX1   \n",
       "\n",
       "                             genenames  \n",
       "0                    CACFD1 CD247 CZIB  \n",
       "1                   RBFOX1 FARP2 COPRS  \n",
       "2                        EHBP1L1 RRP7A  \n",
       "3                         TREML2 DNM1L  \n",
       "4      VIPAS39 VPS33B MATN4 MATN1 ARG2  \n",
       "...                                ...  \n",
       "15321                    ZNF195 ZNF718  \n",
       "15322   MBOAT2 ABHD2 MORC1 LAMP5 MORC2  \n",
       "15323                      ASPDH ZNG1F  \n",
       "15324      ARID3C CELA2A ARID3B ARID3A  \n",
       "15325         TTC39A PRELP DUOX1 PUF60  \n",
       "\n",
       "[15326 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!uv run complexes_rawCSV_to_df.py hu.MAP3.0_complexes_wConfidenceScores_total15326_wGenenames_20240922InOrderMatched.csv\n",
    "import pandas as pd\n",
    "rd3_df = pd.read_pickle('raw_complexes_pickled_df.pkl')\n",
    "rd3_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e55a6-7064-430a-bacd-c9fbb7d2f9fe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a5e9d-b2a9-4085-8fde-3774fc6b5bbf",
   "metadata": {},
   "source": [
    "## Analyze complexes in hu.MAP 2 vs. hu.MAP3 for a protein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73616bb2-04f8-44ca-b6dc-2220c229eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"ROGDI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a71f0-4543-4f4c-8f8e-5a91b2866a1b",
   "metadata": {},
   "source": [
    "Get complexes that protein is in in both sets of data, adding extra information like done in earlier notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3a0227-c905-4572-97e4-613e396f0f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HuMAP2_ID  Confidence Uniprot_ACCs genenames\n",
      "6   HuMAP2_01834           3       Q9Y4E6      WDR7\n",
      "7   HuMAP2_03388           2       Q9Y485     DMXL1\n",
      "8   HuMAP2_03388           2       Q9GZN7     ROGDI\n",
      "9   HuMAP2_03388           2       Q8TDJ6     DMXL2\n",
      "10  HuMAP2_03388           2       Q9Y4E6      WDR7\n",
      " \n",
      "          HuMAP3_ID  ComplexConfidence Uniprot_ACCs genenames\n",
      "297  huMAP3_13872.1                  6       Q8NI08     NCOA7\n",
      "298  huMAP3_13872.1                  6       Q8TDJ6     DMXL2\n",
      "299  huMAP3_13872.1                  6       Q9GZN7     ROGDI\n",
      "300  huMAP3_13872.1                  6       Q9Y485     DMXL1\n",
      "301  huMAP3_13872.1                  6       Q9Y4E6      WDR7\n"
     ]
    }
   ],
   "source": [
    "# Next few cells will run the query collecting all complexes it occurs with and adding details for both datasets\n",
    "pattern = fr'\\b{search_term}\\b' # Create a regex pattern with word boundaries\n",
    "d2_rows_with_term_df = rd2_df[rd2_df['Uniprot_ACCs'].str.contains(pattern, case=False, regex=True) | rd2_df['genenames'].str.contains(pattern, case=False, regex=True)].copy()\n",
    "d3_rows_with_term_df = rd3_df[rd3_df['Uniprot_ACCs'].str.contains(pattern, case=False, regex=True) | rd3_df['genenames'].str.contains(pattern, case=False, regex=True)].copy()\n",
    "# make the dataframe have each row be a single protein\n",
    "# to prepare to use pandas `explode()` to do that, first make the content in be lists\n",
    "d2_rows_with_term_df['Uniprot_ACCs'] = d2_rows_with_term_df['Uniprot_ACCs'].str.split()\n",
    "d3_rows_with_term_df['Uniprot_ACCs'] = d3_rows_with_term_df['Uniprot_ACCs'].str.split()\n",
    "d2_rows_with_term_df['genenames'] = d2_rows_with_term_df['genenames'].str.split()\n",
    "d3_rows_with_term_df['genenames'] = d3_rows_with_term_df['genenames'].str.split()\n",
    "# Now use explode to create a new row for each element in both columns\n",
    "df2_expanded = d2_rows_with_term_df.explode(['Uniprot_ACCs', 'genenames']).copy()\n",
    "df3_expanded = d3_rows_with_term_df.explode(['Uniprot_ACCs', 'genenames']).copy()\n",
    "# Reset the index \n",
    "df2_expanded = df2_expanded.reset_index(drop=True)\n",
    "df3_expanded = df3_expanded.reset_index(drop=True)\n",
    "# Display the first few rows of the expanded dataframe\n",
    "print(df2_expanded.tail())\n",
    "print(\" \")\n",
    "print(df3_expanded.tail())\n",
    "# Next add extra information from UniProt for each protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb2d4bb-f6ae-47f2-b978-2dbc76c76a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell makes lookup table with the extra information; it takes a while to run & so is in a cell on its own to save time during development\n",
    "df_expanded = pd.concat([df2_expanded,df3_expanded])\n",
    "%run -i make_lookup_table_for_extra_info4complexes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2245de-2b7a-4b9b-97f0-6e8a15932644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use collected information to enhance the dataframes\n",
    "pn_dict = {k: v['protein_name'] for k, v in lookup_dict.items()}\n",
    "disease_dict = {k: v['disease'] for k, v in lookup_dict.items()}\n",
    "synonyms_dict = {k: v['synonyms'] for k, v in lookup_dict.items()}\n",
    "df3_expanded['synonyms'] = df3_expanded['Uniprot_ACCs'].map(synonyms_dict)\n",
    "df3_expanded['protein_name'] = df3_expanded['Uniprot_ACCs'].map(pn_dict)\n",
    "df3_expanded['disease'] = df3_expanded['Uniprot_ACCs'].map(disease_dict)\n",
    "conf_val2text_dict = {\n",
    "    1: 'Extremely High',\n",
    "    2: 'Very High',\n",
    "    3: 'High',\n",
    "    4: 'Moderate High',\n",
    "    5: 'Medium High',\n",
    "    6: 'Medium'\n",
    "}\n",
    "# Use vectorized mapping to convert confidence values to text\n",
    "df3_expanded['ComplexConfidence'] = df3_expanded['ComplexConfidence'].map(conf_val2text_dict)\n",
    "base_uniprot_url = 'https://www.uniprot.org/uniprotkb/'\n",
    "df3_expanded = df3_expanded.assign(Link=base_uniprot_url + df3_expanded['Uniprot_ACCs'])\n",
    "\n",
    "# do same for 2.0 data\n",
    "df2_expanded['synonyms'] = df2_expanded['Uniprot_ACCs'].map(synonyms_dict)\n",
    "df2_expanded['protein_name'] = df2_expanded['Uniprot_ACCs'].map(pn_dict)\n",
    "df2_expanded['disease'] = df2_expanded['Uniprot_ACCs'].map(disease_dict)\n",
    "conf_val2text_dict = {\n",
    "    1: 'Extremely High',\n",
    "    2: 'Very High',\n",
    "    3: 'High',\n",
    "    4: 'Moderate High',\n",
    "    5: 'Medium High',\n",
    "    6: 'Medium'\n",
    "}\n",
    "# Use vectorized mapping to convert confidence values to text\n",
    "df2_expanded['Confidence'] = df2_expanded['Confidence'].map(conf_val2text_dict)\n",
    "base_uniprot_url = 'https://www.uniprot.org/uniprotkb/'\n",
    "df2_expanded = df2_expanded.assign(Link=base_uniprot_url + df2_expanded['Uniprot_ACCs'])\n",
    "# Display the first few rows of the expanded dataframe\n",
    "#display(df2_expanded.tail())\n",
    "#display(df3_expanded.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea171c-4da2-4920-a9a5-1cdb6037f0df",
   "metadata": {},
   "source": [
    "Saving that as tab-separated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f6a150-c697-4353-9291-d2d425dada57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "df2_expanded.to_csv(f'{search_term}_complexesHUMAP2{now.strftime(\"_%Y_%m_%d\")}.tsv', sep='\\t',index = False) \n",
    "df3_expanded.to_csv(f'{search_term}_complexesHUMAP3{now.strftime(\"_%Y_%m_%d\")}.tsv', sep='\\t',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e482973-4420-4c9c-a599-dccd80a62d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGDI_complexesHUMAP2_2024_12_02.tsv\n"
     ]
    }
   ],
   "source": [
    "ls *_complexesHUMAP2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d125e5-3ea6-4eb7-8c4d-2d9d07ffb78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGDI_complexesHUMAP3_2024_12_02.tsv\n"
     ]
    }
   ],
   "source": [
    "ls *_complexesHUMAP3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28dbfb62-d888-4adc-afba-85ca0bd092d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HuMAP2_ID Confidence Uniprot_ACCs genenames        synonyms  \\\n",
      "6   HuMAP2_01834       High       Q9Y4E6      WDR7  KIAA0541; TRAG   \n",
      "7   HuMAP2_03388  Very High       Q9Y485     DMXL1             XL1   \n",
      "8   HuMAP2_03388  Very High       Q9GZN7     ROGDI   None reported   \n",
      "9   HuMAP2_03388  Very High       Q8TDJ6     DMXL2        KIAA0856   \n",
      "10  HuMAP2_03388  Very High       Q9Y4E6      WDR7  KIAA0541; TRAG   \n",
      "\n",
      "                      protein_name  \\\n",
      "6   WD repeat-containing protein 7   \n",
      "7               DmX-like protein 1   \n",
      "8            Protein rogdi homolog   \n",
      "9               DmX-like protein 2   \n",
      "10  WD repeat-containing protein 7   \n",
      "\n",
      "                                              disease  \\\n",
      "6                                       None reported   \n",
      "7                                       None reported   \n",
      "8                        Kohlschuetter-Toenz syndrome   \n",
      "9   Polyendocrine-polyneuropathy syndrome; Deafnes...   \n",
      "10                                      None reported   \n",
      "\n",
      "                                        Link  \n",
      "6   https://www.uniprot.org/uniprotkb/Q9Y4E6  \n",
      "7   https://www.uniprot.org/uniprotkb/Q9Y485  \n",
      "8   https://www.uniprot.org/uniprotkb/Q9GZN7  \n",
      "9   https://www.uniprot.org/uniprotkb/Q8TDJ6  \n",
      "10  https://www.uniprot.org/uniprotkb/Q9Y4E6  \n",
      "          HuMAP3_ID ComplexConfidence Uniprot_ACCs genenames        synonyms  \\\n",
      "297  huMAP3_13872.1            Medium       Q8NI08     NCOA7  ERAP140; ESNA1   \n",
      "298  huMAP3_13872.1            Medium       Q8TDJ6     DMXL2        KIAA0856   \n",
      "299  huMAP3_13872.1            Medium       Q9GZN7     ROGDI   None reported   \n",
      "300  huMAP3_13872.1            Medium       Q9Y485     DMXL1             XL1   \n",
      "301  huMAP3_13872.1            Medium       Q9Y4E6      WDR7  KIAA0541; TRAG   \n",
      "\n",
      "                       protein_name  \\\n",
      "297  Nuclear receptor coactivator 7   \n",
      "298              DmX-like protein 2   \n",
      "299           Protein rogdi homolog   \n",
      "300              DmX-like protein 1   \n",
      "301  WD repeat-containing protein 7   \n",
      "\n",
      "                                               disease  \\\n",
      "297                                      None reported   \n",
      "298  Polyendocrine-polyneuropathy syndrome; Deafnes...   \n",
      "299                       Kohlschuetter-Toenz syndrome   \n",
      "300                                      None reported   \n",
      "301                                      None reported   \n",
      "\n",
      "                                         Link  \n",
      "297  https://www.uniprot.org/uniprotkb/Q8NI08  \n",
      "298  https://www.uniprot.org/uniprotkb/Q8TDJ6  \n",
      "299  https://www.uniprot.org/uniprotkb/Q9GZN7  \n",
      "300  https://www.uniprot.org/uniprotkb/Q9Y485  \n",
      "301  https://www.uniprot.org/uniprotkb/Q9Y4E6  \n"
     ]
    }
   ],
   "source": [
    "print(df2_expanded.tail())\n",
    "print(df3_expanded.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8549fa7-55d7-4148-bd7f-063c68b273c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum group size in df2_expanded (HuMAP2_ID): 5\n",
      "Maximum group size in df3_expanded (HuMAP3_ID): 56\n",
      "df3_expanded has the larger maximum group size (56)\n",
      "\n",
      "Groups with maximum size in df2_expanded:\n",
      "HuMAP2_ID\n",
      "HuMAP2_01834    5\n",
      "dtype: int64\n",
      "\n",
      "Groups with maximum size in df3_expanded:\n",
      "HuMAP3_ID\n",
      "huMAP3_12042.1    56\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the size of each group in df2_expanded\n",
    "df2_group_sizes = df2_expanded.groupby('HuMAP2_ID').size()\n",
    "df2_max_size = df2_group_sizes.max()\n",
    "\n",
    "# Get the size of each group in df3_expanded\n",
    "df3_group_sizes = df3_expanded.groupby('HuMAP3_ID').size()\n",
    "df3_max_size = df3_group_sizes.max()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Maximum group size in df2_expanded (HuMAP2_ID): {df2_max_size}\")\n",
    "print(f\"Maximum group size in df3_expanded (HuMAP3_ID): {df3_max_size}\")\n",
    "\n",
    "# Determine which has the larger maximum group size\n",
    "if df2_max_size > df3_max_size:\n",
    "    print(f\"df2_expanded has the larger maximum group size ({df2_max_size})\")\n",
    "elif df3_max_size > df2_max_size:\n",
    "    print(f\"df3_expanded has the larger maximum group size ({df3_max_size})\")\n",
    "else:\n",
    "    print(\"Both dataframes have the same maximum group size\")\n",
    "\n",
    "# Optionally, to see which groups have these maximum sizes:\n",
    "print(\"\\nGroups with maximum size in df2_expanded:\")\n",
    "print(df2_group_sizes[df2_group_sizes == df2_max_size])\n",
    "print(\"\\nGroups with maximum size in df3_expanded:\")\n",
    "print(df3_group_sizes[df3_group_sizes == df3_max_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0b7becb-17bb-4cfa-827f-3dc3144460f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuMAP2_ID\n",
       "HuMAP2_01148    2\n",
       "HuMAP2_01834    5\n",
       "HuMAP2_03388    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_group_sizes = df2_expanded.groupby('HuMAP2_ID').size()\n",
    "df2_group_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa0f6ad8-11e3-487a-a41a-3a01680fabd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuMAP3_ID\n",
       "huMAP3_01501.1    15\n",
       "huMAP3_01899.1    11\n",
       "huMAP3_03469.1     4\n",
       "huMAP3_05952.1     2\n",
       "huMAP3_07099.1    18\n",
       "huMAP3_07329.1    20\n",
       "huMAP3_08381.1    20\n",
       "huMAP3_08678.1    22\n",
       "huMAP3_09242.1    20\n",
       "huMAP3_09477.1    26\n",
       "huMAP3_09873.1    24\n",
       "huMAP3_10511.1    35\n",
       "huMAP3_12042.1    56\n",
       "huMAP3_13872.1    29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_group_sizes = df3_expanded.groupby('HuMAP3_ID').size()\n",
    "df3_group_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fafb8072-0db9-4891-b722-9b2c3802ad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted HuMAP2_ID group sizes:\n",
      "HuMAP2_ID\n",
      "HuMAP2_01834    5\n",
      "HuMAP2_03388    4\n",
      "HuMAP2_01148    2\n",
      "dtype: int64\n",
      "\n",
      "Sorted HuMAP3_ID group sizes:\n",
      "HuMAP3_ID\n",
      "huMAP3_12042.1    56\n",
      "huMAP3_10511.1    35\n",
      "huMAP3_13872.1    29\n",
      "huMAP3_09477.1    26\n",
      "huMAP3_09873.1    24\n",
      "huMAP3_08678.1    22\n",
      "huMAP3_09242.1    20\n",
      "huMAP3_07329.1    20\n",
      "huMAP3_08381.1    20\n",
      "huMAP3_07099.1    18\n",
      "huMAP3_01501.1    15\n",
      "huMAP3_01899.1    11\n",
      "huMAP3_03469.1     4\n",
      "huMAP3_05952.1     2\n",
      "dtype: int64\n",
      "\n",
      "Correspondence by size rank:\n",
      "       HuMAP2_ID  size_df2  rank       HuMAP3_ID  size_df3\n",
      "0   HuMAP2_01834       5.0     1  huMAP3_12042.1        56\n",
      "1   HuMAP2_03388       4.0     2  huMAP3_10511.1        35\n",
      "2   HuMAP2_01148       2.0     3  huMAP3_13872.1        29\n",
      "3            NaN       NaN     4  huMAP3_09477.1        26\n",
      "4            NaN       NaN     5  huMAP3_09873.1        24\n",
      "5            NaN       NaN     6  huMAP3_08678.1        22\n",
      "6            NaN       NaN     7  huMAP3_09242.1        20\n",
      "7            NaN       NaN     8  huMAP3_07329.1        20\n",
      "8            NaN       NaN     9  huMAP3_08381.1        20\n",
      "9            NaN       NaN    10  huMAP3_07099.1        18\n",
      "10           NaN       NaN    11  huMAP3_01501.1        15\n",
      "11           NaN       NaN    12  huMAP3_01899.1        11\n",
      "12           NaN       NaN    13  huMAP3_03469.1         4\n",
      "13           NaN       NaN    14  huMAP3_05952.1         2\n",
      "\n",
      "Do all sizes match?\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Sort groups by size for both dataframes\n",
    "df2_sorted = df2_expanded.groupby('HuMAP2_ID').size().sort_values(ascending=False)\n",
    "df3_sorted = df3_expanded.groupby('HuMAP3_ID').size().sort_values(ascending=False)\n",
    "\n",
    "# Print the sorted group sizes\n",
    "print(\"Sorted HuMAP2_ID group sizes:\")\n",
    "print(df2_sorted)\n",
    "print(\"\\nSorted HuMAP3_ID group sizes:\")\n",
    "print(df3_sorted)\n",
    "\n",
    "# Create a DataFrame to show the correspondences\n",
    "# First, convert the series to dataframes\n",
    "df2_ranks = df2_sorted.reset_index()\n",
    "df3_ranks = df3_sorted.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df2_ranks.columns = ['HuMAP2_ID', 'size_df2']\n",
    "df3_ranks.columns = ['HuMAP3_ID', 'size_df3']\n",
    "\n",
    "# Add rank columns\n",
    "df2_ranks['rank'] = range(1, len(df2_ranks) + 1)\n",
    "df3_ranks['rank'] = range(1, len(df3_ranks) + 1)\n",
    "\n",
    "# Merge the rankings side by side\n",
    "correspondence = pd.merge(\n",
    "    df2_ranks,\n",
    "    df3_ranks,\n",
    "    on='rank',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Sort by rank for clear viewing\n",
    "correspondence = correspondence.sort_values('rank')\n",
    "\n",
    "print(\"\\nCorrespondence by size rank:\")\n",
    "print(correspondence)\n",
    "\n",
    "# To verify if the sizes match exactly\n",
    "print(\"\\nDo all sizes match?\")\n",
    "print((correspondence['size_df2'] == correspondence['size_df3']).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5662a565-09b7-4d8d-ad44-5caa570b6597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 correspondences:\n",
      "      HuMAP2_ID  size_df2  rank       HuMAP3_ID  size_df3\n",
      "0  HuMAP2_01834       5.0     1  huMAP3_12042.1        56\n",
      "1  HuMAP2_03388       4.0     2  huMAP3_10511.1        35\n",
      "2  HuMAP2_01148       2.0     3  huMAP3_13872.1        29\n",
      "3           NaN       NaN     4  huMAP3_09477.1        26\n",
      "4           NaN       NaN     5  huMAP3_09873.1        24\n",
      "5           NaN       NaN     6  huMAP3_08678.1        22\n",
      "6           NaN       NaN     7  huMAP3_09242.1        20\n",
      "7           NaN       NaN     8  huMAP3_07329.1        20\n",
      "8           NaN       NaN     9  huMAP3_08381.1        20\n",
      "9           NaN       NaN    10  huMAP3_07099.1        18\n"
     ]
    }
   ],
   "source": [
    "# Show only top N correspondences (e.g., top 10)\n",
    "N = 10\n",
    "print(f\"\\nTop {N} correspondences:\")\n",
    "print(correspondence.head(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3cd9053-7ced-42e7-abe3-85855cbdce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondence by size rank:\n",
      "       HuMAP2_ID  size_df2  rank       HuMAP3_ID  size_df3\n",
      "HuMAP2_01834       5     1  huMAP3_12042.1       56\n",
      "HuMAP2_03388       4     2  huMAP3_10511.1       35\n",
      "HuMAP2_01148       2     3  huMAP3_13872.1       29\n",
      "                         4  huMAP3_09477.1       26\n",
      "                         5  huMAP3_09873.1       24\n",
      "                         6  huMAP3_08678.1       22\n",
      "                         7  huMAP3_09242.1       20\n",
      "                         8  huMAP3_07329.1       20\n",
      "                         9  huMAP3_08381.1       20\n",
      "                        10  huMAP3_07099.1       18\n",
      "                        11  huMAP3_01501.1       15\n",
      "                        12  huMAP3_01899.1       11\n",
      "                        13  huMAP3_03469.1        4\n",
      "                        14  huMAP3_05952.1        2\n"
     ]
    }
   ],
   "source": [
    "# Cell above close but this looks better without the `3           NaN       NaN     `-like parts\n",
    "# Sort by rank\n",
    "correspondence = correspondence.sort_values('rank')\n",
    "\n",
    "# Convert sizes to integers\n",
    "correspondence['size_df2'] = correspondence['size_df2'].fillna(0).astype(int)\n",
    "correspondence['size_df3'] = correspondence['size_df3'].fillna(0).astype(int)\n",
    "\n",
    "# Create custom string representation\n",
    "print(\"\\nCorrespondence by size rank:\")\n",
    "print(\"       HuMAP2_ID  size_df2  rank       HuMAP3_ID  size_df3\")\n",
    "for _, row in correspondence.iterrows():\n",
    "    humap2_str = str(row['HuMAP2_ID']).ljust(12) if not pd.isna(row['HuMAP2_ID']) else \" \" * 12\n",
    "    size2_str = str(row['size_df2']).rjust(8) if row['size_df2'] != 0 else \" \" * 8\n",
    "    rank_str = str(row['rank']).rjust(6)\n",
    "    humap3_str = str(row['HuMAP3_ID']).ljust(15)\n",
    "    size3_str = str(row['size_df3']).rjust(8)\n",
    "    \n",
    "    if pd.isna(row['HuMAP2_ID']):\n",
    "        print(f\"{' ' * 20}{rank_str}  {humap3_str}{size3_str}\")\n",
    "    else:\n",
    "        print(f\"{humap2_str}{size2_str}{rank_str}  {humap3_str}{size3_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce535cd5-0326-4016-954d-194b4304625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondence by size rank:\n",
      "       HuMAP2_ID size_df2  rank       HuMAP3_ID  size_df3\n",
      "0   HuMAP2_01834        5     1  huMAP3_12042.1        56\n",
      "1   HuMAP2_03388        4     2  huMAP3_10511.1        35\n",
      "2   HuMAP2_01148        2     3  huMAP3_13872.1        29\n",
      "3                             4  huMAP3_09477.1        26\n",
      "4                             5  huMAP3_09873.1        24\n",
      "5                             6  huMAP3_08678.1        22\n",
      "6                             7  huMAP3_09242.1        20\n",
      "7                             8  huMAP3_07329.1        20\n",
      "8                             9  huMAP3_08381.1        20\n",
      "9                            10  huMAP3_07099.1        18\n",
      "10                           11  huMAP3_01501.1        15\n",
      "11                           12  huMAP3_01899.1        11\n",
      "12                           13  huMAP3_03469.1         4\n",
      "13                           14  huMAP3_05952.1         2\n"
     ]
    }
   ],
   "source": [
    "# Alternative to cell just above using Pandas' built-in display options\n",
    "# Convert sizes to integers\n",
    "# Convert sizes to integers\n",
    "correspondence['size_df2'] = correspondence['size_df2'].fillna(0).astype(int)\n",
    "correspondence['size_df3'] = correspondence['size_df3'].fillna(0).astype(int)\n",
    "\n",
    "# Replace NaN values with empty strings for display\n",
    "display_df = correspondence.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\\nCorrespondence by size rank:\")\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f1fa98e-9234-454f-af7e-30fbc8f324fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondence by size rank:\n",
      "   HuMAP2_ID size_df2  rank      HuMAP3_ID  size_df3\n",
      "HuMAP2_01834        5     1 huMAP3_12042.1        56\n",
      "HuMAP2_03388        4     2 huMAP3_10511.1        35\n",
      "HuMAP2_01148        2     3 huMAP3_13872.1        29\n",
      "                          4 huMAP3_09477.1        26\n",
      "                          5 huMAP3_09873.1        24\n",
      "                          6 huMAP3_08678.1        22\n",
      "                          7 huMAP3_09242.1        20\n",
      "                          8 huMAP3_07329.1        20\n",
      "                          9 huMAP3_08381.1        20\n",
      "                         10 huMAP3_07099.1        18\n",
      "                         11 huMAP3_01501.1        15\n",
      "                         12 huMAP3_01899.1        11\n",
      "                         13 huMAP3_03469.1         4\n",
      "                         14 huMAP3_05952.1         2\n"
     ]
    }
   ],
   "source": [
    "# Even cleaner Alternative to cell just above using Pandas' built-in display options\n",
    "# Convert sizes to integers\n",
    "correspondence['size_df2'] = correspondence['size_df2'].fillna(0).astype(int)\n",
    "correspondence['size_df3'] = correspondence['size_df3'].fillna(0).astype(int)\n",
    "\n",
    "# Replace NaN values with empty strings for display\n",
    "display_df = correspondence.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\\nCorrespondence by size rank:\")\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c753a40-6220-4fc4-87ed-7da4b9919676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size rank for version 2 vs. 3 with 'improvement' in version 3 indicated:\n",
      "   HuMAP2_ID size_df2  rank      HuMAP3_ID  size_df3 improvement_in_v3\n",
      "HuMAP2_01834        5     1 huMAP3_12042.1        56                51\n",
      "HuMAP2_03388        4     2 huMAP3_10511.1        35                31\n",
      "HuMAP2_01148        2     3 huMAP3_13872.1        29                27\n",
      "                          4 huMAP3_09477.1        26                na\n",
      "                          5 huMAP3_09873.1        24                na\n",
      "                          6 huMAP3_08678.1        22                na\n",
      "                          7 huMAP3_09242.1        20                na\n",
      "                          8 huMAP3_07329.1        20                na\n",
      "                          9 huMAP3_08381.1        20                na\n",
      "                         10 huMAP3_07099.1        18                na\n",
      "                         11 huMAP3_01501.1        15                na\n",
      "                         12 huMAP3_01899.1        11                na\n",
      "                         13 huMAP3_03469.1         4                na\n",
      "                         14 huMAP3_05952.1         2                na\n"
     ]
    }
   ],
   "source": [
    "# USe 'even cleaner Alternative' to do what cell below does: # Add a column showing the size difference\n",
    "# Convert sizes to integers\n",
    "# Sort groups by size for both dataframes\n",
    "df2_sorted = df2_expanded.groupby('HuMAP2_ID').size().sort_values(ascending=False)\n",
    "df3_sorted = df3_expanded.groupby('HuMAP3_ID').size().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Create a DataFrame to show the correspondences\n",
    "# First, convert the series to dataframes\n",
    "df2_ranks = df2_sorted.reset_index()\n",
    "df3_ranks = df3_sorted.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df2_ranks.columns = ['HuMAP2_ID', 'size_df2']\n",
    "df3_ranks.columns = ['HuMAP3_ID', 'size_df3']\n",
    "\n",
    "# Add rank columns\n",
    "df2_ranks['rank'] = range(1, len(df2_ranks) + 1)\n",
    "df3_ranks['rank'] = range(1, len(df3_ranks) + 1)\n",
    "\n",
    "# Merge the rankings side by side\n",
    "size_rank_df = pd.merge(\n",
    "    df2_ranks,\n",
    "    df3_ranks,\n",
    "    on='rank',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Sort by rank for clear viewing\n",
    "size_rank_df = size_rank_df.sort_values('rank')\n",
    "# Convert sizes to integers\n",
    "size_rank_df['size_df2'] = size_rank_df['size_df2'].fillna(0).astype(int)\n",
    "size_rank_df['size_df3'] = size_rank_df['size_df3'].fillna(0).astype(int)\n",
    "#size_rank_df['improvement_in_v3'] =  size_rank_df['size_df3'] - size_rank_df['size_df2']\n",
    "size_rank_df['improvement_in_v3'] = size_rank_df.apply(\n",
    "    lambda row: row['size_df3'] - row['size_df2'] if not pd.isna(row['HuMAP2_ID']) else 'na', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Replace NaN values with empty strings for display\n",
    "display_df = size_rank_df.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"\\nSize rank for version 2 vs. 3 with 'improvement' in version 3 indicated:\")\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce63ed3f-c8f9-40d6-bf67-47d7559a41ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size rank for version 2 vs. 3 with 'improvement' in version 3 and shared items:\n",
      "   HuMAP2_ID size_df2  rank      HuMAP3_ID  size_df3 improvement_in_v3 shared_items\n",
      "HuMAP2_01834        5     1 huMAP3_12042.1        56                51            4\n",
      "HuMAP2_03388        4     2 huMAP3_10511.1        35                31            3\n",
      "HuMAP2_01148        2     3 huMAP3_13872.1        29                27            2\n",
      "                          4 huMAP3_09477.1        26                na           na\n",
      "                          5 huMAP3_09873.1        24                na           na\n",
      "                          6 huMAP3_08678.1        22                na           na\n",
      "                          7 huMAP3_09242.1        20                na           na\n",
      "                          8 huMAP3_07329.1        20                na           na\n",
      "                          9 huMAP3_08381.1        20                na           na\n",
      "                         10 huMAP3_07099.1        18                na           na\n",
      "                         11 huMAP3_01501.1        15                na           na\n",
      "                         12 huMAP3_01899.1        11                na           na\n",
      "                         13 huMAP3_03469.1         4                na           na\n",
      "                         14 huMAP3_05952.1         2                na           na\n"
     ]
    }
   ],
   "source": [
    "## Add number of shared items\n",
    "# First, get unique HuMAP3_IDs\n",
    "humap3_ids = df3_expanded['HuMAP3_ID'].unique()\n",
    "shared_items_dict = {}\n",
    "\n",
    "df2_groups = df2_expanded.groupby('HuMAP2_ID')['Uniprot_ACCs'].apply(list)\n",
    "df3_groups = df3_expanded.groupby('HuMAP3_ID')['Uniprot_ACCs'].apply(list)\n",
    "\n",
    "# Convert lists to sets\n",
    "df2_sets = {idx: set(ids) for idx, ids in df2_groups.items()}\n",
    "df3_sets = {idx: set(ids) for idx, ids in df3_groups.items()}\n",
    "\n",
    "# Function to check shared items\n",
    "def get_shared_items(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "# Populate the nested dictionary from the dataframes - keys for the `shared_items_dict` will be the 'HuMAP2_ID' ids with the values for each being a dictionary with the 'HuMAP3_ID' as the keys with the value of number of shared items\n",
    "for _, row in df2_expanded.iterrows():\n",
    "    humap2_id = row['HuMAP2_ID']\n",
    "    \n",
    "    # Initialize nested dictionary if HuMAP2_ID doesn't exist\n",
    "    if humap2_id not in shared_items_dict:\n",
    "        shared_items_dict[humap2_id] = {}\n",
    "        # For each unique HuMAP3_ID, find shared items\n",
    "        for humap3_id in humap3_ids:\n",
    "            shared_ids = get_shared_items(df2_sets[humap2_id], df3_sets[humap3_id])\n",
    "            shared_items_dict[humap2_id][humap3_id] = len(shared_ids)\n",
    "\n",
    "'''# Debugging function to print out the dictionary\n",
    "def print_shared_items_debug(humap2_id):\n",
    "    print(f\"\\nDebugging shared items for {humap2_id}:\")\n",
    "    if humap2_id in shared_items_dict:\n",
    "        for humap3_id in humap3_ids:\n",
    "            print(f\"  {humap3_id}: {shared_items_dict[humap2_id][humap3_id]} shared Uniprot_ACCs\")\n",
    "    else:\n",
    "        print(\"  No entries found\")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add a new column for shared items to the correspondence DataFrame\n",
    "#size_rank_df['shared_items'] = size_rank_df['HuMAP2_ID'].map(shared_items_dict).fillna('na')\n",
    "#size_rank_df['shared_items'] = size_rank_df['shared_items'].apply(lambda x: int(x) if isinstance(x, (int, float)) and not pd.isna(x) else 'na')\n",
    "#size_rank_df['shared_items'] = size_rank_df['HuMAP2_ID'].map(shared_items_dict).apply(lambda x: int(x) if isinstance(x, (int, float)) and not pd.isna(x) else 'na')\n",
    "def get_shared_items(ids):\n",
    "    humap2_id = ids.iloc[0]\n",
    "    humap3_id = ids.iloc[1]\n",
    "    if humap2_id in shared_items_dict:\n",
    "        return shared_items_dict[humap2_id][humap3_id]\n",
    "    return 'na'\n",
    "\n",
    "# Optional: Add debugging print statement for the first few rows\n",
    "#print_shared_items_debug('HuMAP2_01834')\n",
    "\n",
    "# Apply the new method to get shared items\n",
    "size_rank_df['shared_items'] = size_rank_df[['HuMAP2_ID','HuMAP3_ID']].apply(get_shared_items, axis=1)\n",
    "\n",
    "# When preparing for display, modify the display_df creation\n",
    "display_df = size_rank_df.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "display_df['shared_items'] = display_df['shared_items'].replace('', 'na')\n",
    "\n",
    "# Update print statement\n",
    "print(\"\\nSize rank for version 2 vs. 3 with 'improvement' in version 3 and shared items:\")\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e5c057-24b9-47d9-91f1-5fa9aa713891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Size rank for version 2 vs. 3 with improvement, shared items, and coverage:\n",
      "   HuMAP2_ID size_df2  rank      HuMAP3_ID  size_df3 improvement_in_v3 shared_items coverage_ratio\n",
      "HuMAP2_01834        5     1 huMAP3_12042.1        56                51            4           80.0\n",
      "HuMAP2_03388        4     2 huMAP3_10511.1        35                31            3           75.0\n",
      "HuMAP2_01148        2     3 huMAP3_13872.1        29                27            2          100.0\n",
      "                          4 huMAP3_09477.1        26                na           na             na\n",
      "                          5 huMAP3_09873.1        24                na           na             na\n",
      "                          6 huMAP3_08678.1        22                na           na             na\n",
      "                          7 huMAP3_09242.1        20                na           na             na\n",
      "                          8 huMAP3_07329.1        20                na           na             na\n",
      "                          9 huMAP3_08381.1        20                na           na             na\n",
      "                         10 huMAP3_07099.1        18                na           na             na\n",
      "                         11 huMAP3_01501.1        15                na           na             na\n",
      "                         12 huMAP3_01899.1        11                na           na             na\n",
      "                         13 huMAP3_03469.1         4                na           na             na\n",
      "                         14 huMAP3_05952.1         2                na           na             na\n"
     ]
    }
   ],
   "source": [
    "# Add a new column for coverage ratio\n",
    "size_rank_df['coverage_ratio'] = size_rank_df.apply(\n",
    "    lambda row: round(row['shared_items'] / min(row['size_df2'], row['size_df3']) * 100, 2) \n",
    "    if row['shared_items'] != 'na' and row['size_df2'] > 0 and row['size_df3'] > 0 \n",
    "    else 'na', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Modify display preparation\n",
    "display_df = size_rank_df.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "display_df['shared_items'] = display_df['shared_items'].replace('', 'na')\n",
    "\n",
    "# Update print statement\n",
    "print(\"\\nSize rank for version 2 vs. 3 with improvement, shared items, and coverage:\")\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d6af439-54c4-4f1e-9288-1fce2dece282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondence by size rank (without NaN rows):\n",
      "      HuMAP2_ID  size_df2  rank       HuMAP3_ID  size_df3\n",
      "0  HuMAP2_01834       5.0     1  huMAP3_12042.1        56\n",
      "1  HuMAP2_03388       4.0     2  huMAP3_10511.1        35\n",
      "2  HuMAP2_01148       2.0     3  huMAP3_13872.1        29\n",
      "\n",
      "Size Correspondences (HuMAP2_ID -> HuMAP3_ID):\n",
      "HuMAP2_01834 corresponds to huMAP3_12042.1 (sizes: 5 and 56)\n",
      "HuMAP2_03388 corresponds to huMAP3_10511.1 (sizes: 4 and 35)\n",
      "HuMAP2_01148 corresponds to huMAP3_13872.1 (sizes: 2 and 29)\n"
     ]
    }
   ],
   "source": [
    "# Merge the rankings side by side in simple output\n",
    "ssize_rank_df = pd.merge(\n",
    "    df2_ranks,\n",
    "    df3_ranks,\n",
    "    on='rank',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Sort by rank and drop rows where either side is completely NaN\n",
    "ssize_rank_df = ssize_rank_df.sort_values('rank')\n",
    "ssize_rank_df_clean = ssize_rank_df.dropna(subset=['HuMAP2_ID', 'HuMAP3_ID'], how='any')\n",
    "\n",
    "print(\"\\nCorrespondence by size rank (without NaN rows):\")\n",
    "print(ssize_rank_df_clean)\n",
    "\n",
    "# Get the correspondences as a dictionary or series\n",
    "# This will match IDs based on their size rankings\n",
    "size_corr_dict = dict(zip(ssize_rank_df_clean['HuMAP2_ID'], ssize_rank_df_clean['HuMAP3_ID']))\n",
    "\n",
    "print(\"\\nSize Correspondences (HuMAP2_ID -> HuMAP3_ID):\")\n",
    "for humap2, humap3 in size_corr_dict.items():\n",
    "    print(f\"{humap2} corresponds to {humap3} (sizes: {int(ssize_rank_df_clean[ssize_rank_df_clean['HuMAP2_ID'] == humap2]['size_df2'].iloc[0])} and {int(ssize_rank_df_clean[ssize_rank_df_clean['HuMAP3_ID'] == humap3]['size_df3'].iloc[0])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611d8d0-d701-4733-a87d-34af04c2e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa96824f-142d-419e-946f-029ea7bc5d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches (with at least 2 shared items):\n",
      "\n",
      "Detailed view:\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_03469.1\n",
      "Weighted similarity: 1.000\n",
      "Shared items: 4 (out of 4 in HuMAP2 and 4 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01148 <-> HuMAP3 huMAP3_05952.1\n",
      "Weighted similarity: 1.000\n",
      "Shared items: 2 (out of 2 in HuMAP2 and 2 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_03469.1\n",
      "Weighted similarity: 0.800\n",
      "Shared items: 4 (out of 5 in HuMAP2 and 4 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01148 <-> HuMAP3 huMAP3_03469.1\n",
      "Weighted similarity: 0.500\n",
      "Shared items: 2 (out of 2 in HuMAP2 and 4 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_05952.1\n",
      "Weighted similarity: 0.500\n",
      "Shared items: 2 (out of 4 in HuMAP2 and 2 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_05952.1\n",
      "Weighted similarity: 0.400\n",
      "Shared items: 2 (out of 5 in HuMAP2 and 2 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_01899.1\n",
      "Weighted similarity: 0.257\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 11 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_01899.1\n",
      "Weighted similarity: 0.243\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 11 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_01501.1\n",
      "Weighted similarity: 0.191\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 15 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_01501.1\n",
      "Weighted similarity: 0.184\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 15 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_07099.1\n",
      "Weighted similarity: 0.161\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 18 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_07099.1\n",
      "Weighted similarity: 0.155\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 18 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_07329.1\n",
      "Weighted similarity: 0.145\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 20 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_08381.1\n",
      "Weighted similarity: 0.145\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 20 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_09242.1\n",
      "Weighted similarity: 0.145\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 20 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_07329.1\n",
      "Weighted similarity: 0.140\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 20 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_08381.1\n",
      "Weighted similarity: 0.140\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 20 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_09242.1\n",
      "Weighted similarity: 0.140\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 20 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_13872.1\n",
      "Weighted similarity: 0.138\n",
      "Shared items: 4 (out of 4 in HuMAP2 and 29 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_13872.1\n",
      "Weighted similarity: 0.135\n",
      "Shared items: 4 (out of 5 in HuMAP2 and 29 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_08678.1\n",
      "Weighted similarity: 0.132\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 22 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_08678.1\n",
      "Weighted similarity: 0.128\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 22 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_09873.1\n",
      "Weighted similarity: 0.121\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 24 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_09873.1\n",
      "Weighted similarity: 0.118\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 24 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_09477.1\n",
      "Weighted similarity: 0.112\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 26 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_09477.1\n",
      "Weighted similarity: 0.110\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 26 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_10511.1\n",
      "Weighted similarity: 0.084\n",
      "Shared items: 3 (out of 4 in HuMAP2 and 35 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_10511.1\n",
      "Weighted similarity: 0.082\n",
      "Shared items: 3 (out of 5 in HuMAP2 and 35 in HuMAP3)\n",
      "Shared UniProt IDs: Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_03388 <-> HuMAP3 huMAP3_12042.1\n",
      "Weighted similarity: 0.071\n",
      "Shared items: 4 (out of 4 in HuMAP2 and 56 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01834 <-> HuMAP3 huMAP3_12042.1\n",
      "Weighted similarity: 0.071\n",
      "Shared items: 4 (out of 5 in HuMAP2 and 56 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7, Q9Y485, Q9Y4E6\n",
      "\n",
      "HuMAP2 HuMAP2_01148 <-> HuMAP3 huMAP3_13872.1\n",
      "Weighted similarity: 0.069\n",
      "Shared items: 2 (out of 2 in HuMAP2 and 29 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7\n",
      "\n",
      "HuMAP2 HuMAP2_01148 <-> HuMAP3 huMAP3_12042.1\n",
      "Weighted similarity: 0.036\n",
      "Shared items: 2 (out of 2 in HuMAP2 and 56 in HuMAP3)\n",
      "Shared UniProt IDs: Q8TDJ6, Q9GZN7\n",
      "\n",
      "Best match for each HuMAP2_ID:\n",
      "       HuMAP2_ID       HuMAP3_ID  weighted_similarity  shared_items\n",
      "1   HuMAP2_01148  huMAP3_05952.1                  1.0             2\n",
      "6   HuMAP2_01834  huMAP3_03469.1                  0.8             4\n",
      "20  HuMAP2_03388  huMAP3_03469.1                  1.0             4\n",
      "\n",
      "Perfect subset matches (where all members of the smaller set are contained in the larger set):\n",
      "       HuMAP2_ID       HuMAP3_ID  shared_items  total_humap2  total_humap3\n",
      "20  HuMAP2_03388  huMAP3_03469.1             4             4             4\n",
      "1   HuMAP2_01148  huMAP3_05952.1             2             2             2\n",
      "6   HuMAP2_01834  huMAP3_03469.1             4             5             4\n",
      "0   HuMAP2_01148  huMAP3_03469.1             2             2             4\n",
      "21  HuMAP2_03388  huMAP3_05952.1             2             4             2\n",
      "7   HuMAP2_01834  huMAP3_05952.1             2             5             2\n",
      "31  HuMAP2_03388  huMAP3_13872.1             4             4            29\n",
      "30  HuMAP2_03388  huMAP3_12042.1             4             4            56\n",
      "3   HuMAP2_01148  huMAP3_13872.1             2             2            29\n",
      "2   HuMAP2_01148  huMAP3_12042.1             2             2            56\n"
     ]
    }
   ],
   "source": [
    "#Comparing the complex members to better see what corresponds\n",
    "\n",
    "# First get the groups from both dataframes\n",
    "df2_groups = df2_expanded.groupby('HuMAP2_ID')['Uniprot_ACCs'].apply(list)\n",
    "df3_groups = df3_expanded.groupby('HuMAP3_ID')['Uniprot_ACCs'].apply(list)\n",
    "\n",
    "# Convert lists to sets\n",
    "source1_sets = {idx: set(ids) for idx, ids in df2_groups.items()}\n",
    "source2_sets = {idx: set(ids) for idx, ids in df3_groups.items()}\n",
    "\n",
    "def weighted_similarity(set1, set2, weight_jaccard=0.7):\n",
    "    \"\"\"Combined metric that considers both Jaccard similarity and raw intersection size\"\"\"\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    jaccard = len(intersection) / len(union) if union else 0\n",
    "    max_possible_intersection = max(len(set1), len(set2))\n",
    "    normalized_intersection = len(intersection) / max_possible_intersection if max_possible_intersection != 0 else 0\n",
    "    \n",
    "    return weight_jaccard * jaccard + (1 - weight_jaccard) * normalized_intersection\n",
    "\n",
    "# Calculate similarities for all pairs\n",
    "similarities = []\n",
    "for humap2_id, set1 in source1_sets.items():\n",
    "    for humap3_id, set2 in source2_sets.items():\n",
    "        intersection = set1.intersection(set2)\n",
    "        if len(intersection) >= 2:  # Minimum overlap constraint\n",
    "            similarities.append({\n",
    "                'HuMAP2_ID': humap2_id,\n",
    "                'HuMAP3_ID': humap3_id,\n",
    "                'weighted_similarity': weighted_similarity(set1, set2),\n",
    "                'shared_items': len(intersection),\n",
    "                'total_humap2': len(set1),\n",
    "                'total_humap3': len(set2),\n",
    "                'shared_elements': sorted(list(intersection))\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame and sort by similarity\n",
    "df_similarities = pd.DataFrame(similarities)\n",
    "df_similarities = df_similarities.sort_values(['weighted_similarity', 'shared_items'], \n",
    "                                           ascending=[False, False])\n",
    "\n",
    "# Print results\n",
    "print(\"Top matches (with at least 2 shared items):\")\n",
    "print(\"\\nDetailed view:\")\n",
    "for _, row in df_similarities.iterrows():\n",
    "    print(f\"\\nHuMAP2 {row['HuMAP2_ID']} <-> HuMAP3 {row['HuMAP3_ID']}\")\n",
    "    print(f\"Weighted similarity: {row['weighted_similarity']:.3f}\")\n",
    "    print(f\"Shared items: {row['shared_items']} (out of {row['total_humap2']} in HuMAP2 and {row['total_humap3']} in HuMAP3)\")\n",
    "    print(f\"Shared UniProt IDs: {', '.join(row['shared_elements'])}\")\n",
    "\n",
    "# Get best matches for each HuMAP2_ID\n",
    "best_matches = df_similarities.loc[df_similarities.groupby('HuMAP2_ID')['weighted_similarity'].idxmax()]\n",
    "print(\"\\nBest match for each HuMAP2_ID:\")\n",
    "print(best_matches[['HuMAP2_ID', 'HuMAP3_ID', 'weighted_similarity', 'shared_items']].to_string())\n",
    "\n",
    "# Get matches that share all members\n",
    "perfect_subset_matches = df_similarities[\n",
    "    df_similarities.apply(lambda x: \n",
    "        x['shared_items'] == min(x['total_humap2'], x['total_humap3']), \n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "if not perfect_subset_matches.empty:\n",
    "    print(\"\\nPerfect subset matches (where all members of the smaller set are contained in the larger set):\")\n",
    "    print(perfect_subset_matches[['HuMAP2_ID', 'HuMAP3_ID', 'shared_items', \n",
    "                                'total_humap2', 'total_humap3']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9ca8fa9-ca6b-47f6-9639-59baf64e459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondences:\n",
      "\n",
      "HuMAP2: HuMAP2_01834 (size: 5)\n",
      "HuMAP3: huMAP3_12042.1 (size: 56)\n",
      "Shared items: 4\n",
      "Shared elements: {'Q8TDJ6', 'Q9GZN7', 'Q9Y4E6', 'Q9Y485'}\n",
      "Weighted similarity: 0.071\n",
      "\n",
      "HuMAP2: HuMAP2_03388 (size: 4)\n",
      "HuMAP3: huMAP3_10511.1 (size: 35)\n",
      "Shared items: 3\n",
      "Shared elements: {'Q9Y485', 'Q9GZN7', 'Q9Y4E6'}\n",
      "Weighted similarity: 0.084\n",
      "\n",
      "HuMAP2: HuMAP2_01148 (size: 2)\n",
      "HuMAP3: huMAP3_13872.1 (size: 29)\n",
      "Shared items: 2\n",
      "Shared elements: {'Q8TDJ6', 'Q9GZN7'}\n",
      "Weighted similarity: 0.069\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HuMAP2_ID</th>\n",
       "      <th>HuMAP3_ID</th>\n",
       "      <th>HuMAP2_size</th>\n",
       "      <th>HuMAP3_size</th>\n",
       "      <th>shared_items</th>\n",
       "      <th>shared_elements</th>\n",
       "      <th>weighted_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HuMAP2_01834</td>\n",
       "      <td>huMAP3_12042.1</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>{Q8TDJ6, Q9GZN7, Q9Y4E6, Q9Y485}</td>\n",
       "      <td>0.070551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HuMAP2_03388</td>\n",
       "      <td>huMAP3_10511.1</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>{Q9Y485, Q9GZN7, Q9Y4E6}</td>\n",
       "      <td>0.084048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HuMAP2_01148</td>\n",
       "      <td>huMAP3_13872.1</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>{Q8TDJ6, Q9GZN7}</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      HuMAP2_ID       HuMAP3_ID  HuMAP2_size  HuMAP3_size  shared_items  \\\n",
       "0  HuMAP2_01834  huMAP3_12042.1            5           56             4   \n",
       "1  HuMAP2_03388  huMAP3_10511.1            4           35             3   \n",
       "2  HuMAP2_01148  huMAP3_13872.1            2           29             2   \n",
       "\n",
       "                    shared_elements  weighted_similarity  \n",
       "0  {Q8TDJ6, Q9GZN7, Q9Y4E6, Q9Y485}             0.070551  \n",
       "1          {Q9Y485, Q9GZN7, Q9Y4E6}             0.084048  \n",
       "2                  {Q8TDJ6, Q9GZN7}             0.068966  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try combining, because ranked by size probably correct if the number of shared items doesn't get any better if you re-order. Then fall back to ranking by share members\n",
    "# if doesn't seem to work that the shared items never gets any better from ranked size comparisons.\n",
    "# Get the groups from both dataframes\n",
    "df2_groups = df2_expanded.groupby('HuMAP2_ID')['Uniprot_ACCs'].apply(list)\n",
    "df3_groups = df3_expanded.groupby('HuMAP3_ID')['Uniprot_ACCs'].apply(list)\n",
    "\n",
    "# Convert lists to sets\n",
    "source1_sets = {idx: set(ids) for idx, ids in df2_groups.items()}\n",
    "source2_sets = {idx: set(ids) for idx, ids in df3_groups.items()}\n",
    "\n",
    "# Sort groups by size\n",
    "source1_sorted = sorted(source1_sets.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "source2_sorted = sorted(source2_sets.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "# Function to check shared items\n",
    "def get_shared_items(set1, set2):\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "# Function for weighted similarity\n",
    "def weighted_similarity(set1, set2, weight_jaccard=0.7):\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    jaccard = len(intersection) / len(union) if union else 0\n",
    "    max_possible_intersection = max(len(set1), len(set2))\n",
    "    normalized_intersection = len(intersection) / max_possible_intersection if max_possible_intersection != 0 else 0\n",
    "    \n",
    "    return weight_jaccard * jaccard + (1 - weight_jaccard) * normalized_intersection\n",
    "\n",
    "# Correspondence tracking\n",
    "correspondences = []\n",
    "used_source2_indices = set()\n",
    "\n",
    "# First pass: Try to match by ranking and shared items\n",
    "for i, (humap2_id, set1) in enumerate(source1_sorted):\n",
    "    # If we've reached the end of source2 sorted list, break\n",
    "    if i >= len(source2_sorted):\n",
    "        break\n",
    "    \n",
    "    humap3_id, set2 = source2_sorted[i]\n",
    "    \n",
    "    # Check shared items\n",
    "    shared = get_shared_items(set1, set2)\n",
    "    \n",
    "    if len(shared) > 1:  # need greater than 1 and not zero because always going to be greater than zero because every complex has to have query identifier\n",
    "        correspondences.append({\n",
    "            'HuMAP2_ID': humap2_id,\n",
    "            'HuMAP3_ID': humap3_id,\n",
    "            'HuMAP2_size': len(set1),\n",
    "            'HuMAP3_size': len(set2),\n",
    "            'shared_items': len(shared),\n",
    "            'shared_elements': shared,\n",
    "            'weighted_similarity': weighted_similarity(set1, set2)\n",
    "        })\n",
    "        used_source2_indices.add(i)\n",
    "\n",
    "# Second pass: Fallback method for unmatched groups\n",
    "# Create a list of unused source2 indices\n",
    "unused_source2_indices = [\n",
    "    j for j in range(len(source2_sorted)) \n",
    "    if j not in used_source2_indices\n",
    "]\n",
    "\n",
    "# Fallback matching\n",
    "for humap2_id, set1 in source1_sorted:\n",
    "    # Skip if this HuMAP2 ID has already been matched\n",
    "    if any(corr['HuMAP2_ID'] == humap2_id for corr in correspondences):\n",
    "        continue\n",
    "    \n",
    "    # Find best match among unused source2 groups\n",
    "    best_match = None\n",
    "    best_similarity = -1\n",
    "    best_unused_index = -1\n",
    "    \n",
    "    for j in unused_source2_indices:\n",
    "        humap3_id, set2 = source2_sorted[j]\n",
    "        similarity = weighted_similarity(set1, set2)\n",
    "        \n",
    "        if similarity > best_similarity:\n",
    "            best_match = (humap3_id, set2)\n",
    "            best_similarity = similarity\n",
    "            best_unused_index = j\n",
    "    \n",
    "    # Add the best match if found\n",
    "    if best_match:\n",
    "        humap3_id, set2 = best_match\n",
    "        shared = get_shared_items(set1, set2)\n",
    "        \n",
    "        correspondences.append({\n",
    "            'HuMAP2_ID': humap2_id,\n",
    "            'HuMAP3_ID': humap3_id,\n",
    "            'HuMAP2_size': len(set1),\n",
    "            'HuMAP3_size': len(set2),\n",
    "            'shared_items': len(shared),\n",
    "            'shared_elements': shared,\n",
    "            'weighted_similarity': best_similarity\n",
    "        })\n",
    "        \n",
    "        # Remove this index from unused indices\n",
    "        unused_source2_indices.remove(best_unused_index)\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "df_correspondences = pd.DataFrame(correspondences)\n",
    "\n",
    "# Sort by HuMAP2 ID size\n",
    "df_correspondences = df_correspondences.sort_values('HuMAP2_size', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCorrespondences:\")\n",
    "for _, row in df_correspondences.iterrows():\n",
    "    print(f\"\\nHuMAP2: {row['HuMAP2_ID']} (size: {row['HuMAP2_size']})\")\n",
    "    print(f\"HuMAP3: {row['HuMAP3_ID']} (size: {row['HuMAP3_size']})\")\n",
    "    print(f\"Shared items: {row['shared_items']}\")\n",
    "    print(f\"Shared elements: {row['shared_elements']}\")\n",
    "    print(f\"Weighted similarity: {row['weighted_similarity']:.3f}\")\n",
    "\n",
    "# Optional: Save to CSV for further analysis\n",
    "df_correspondences.to_csv('humap_correspondences.csv', index=False)\n",
    "df_correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d60879f0-7be6-4f40-91c9-da6c7b1d002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correspondences (considering shared complex members) for version 2 vs. 3 with improvement, shared items, and coverage ratio:\n",
      "   HuMAP2_ID size_df2  rank      HuMAP3_ID  size_df3 improvement_in_v3 shared_items coverage_ratio\n",
      "HuMAP2_01834        5     1 huMAP3_12042.1        56                51            4           80.0\n",
      "HuMAP2_03388        4     2 huMAP3_10511.1        35                31            3           75.0\n",
      "HuMAP2_01148        2     3 huMAP3_13872.1        29                27            2          100.0\n",
      "                          4 huMAP3_09477.1        26                na           na             na\n",
      "                          5 huMAP3_09873.1        24                na           na             na\n",
      "                          6 huMAP3_08678.1        22                na           na             na\n",
      "                          7 huMAP3_09242.1        20                na           na             na\n",
      "                          8 huMAP3_07329.1        20                na           na             na\n",
      "                          9 huMAP3_08381.1        20                na           na             na\n",
      "                         10 huMAP3_07099.1        18                na           na             na\n",
      "                         11 huMAP3_01501.1        15                na           na             na\n",
      "                         12 huMAP3_01899.1        11                na           na             na\n",
      "                         13 huMAP3_03469.1         4                na           na             na\n",
      "                         14 huMAP3_05952.1         2                na           na             na\n"
     ]
    }
   ],
   "source": [
    "# STILL TROUBLESHOOTING THIS BUT IT IS CLOSE\n",
    "# make summary table (LIKE THE EARLIER ONE) with those correspondences\n",
    "# Create a dictionary to map HuMAP2_ID to its corresponding details\n",
    "correspondences_dict = {\n",
    "    corr['HuMAP2_ID']: {\n",
    "        'HuMAP3_ID': corr['HuMAP3_ID'],\n",
    "        'size_df3': corr['HuMAP3_size'],\n",
    "        'shared_items': corr['shared_items'],\n",
    "        'weighted_similarity': corr['weighted_similarity']\n",
    "    } for corr in correspondences\n",
    "}\n",
    "\n",
    "# Modify the correspondence creation to use this new matching\n",
    "# First, create the base dataframe as before\n",
    "df2_sorted = df2_expanded.groupby('HuMAP2_ID').size().sort_values(ascending=False)\n",
    "df3_sorted = df3_expanded.groupby('HuMAP3_ID').size().sort_values(ascending=False)\n",
    "\n",
    "# Convert the series to dataframes\n",
    "df2_ranks = df2_sorted.reset_index()\n",
    "df3_ranks = df3_sorted.reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "df2_ranks.columns = ['HuMAP2_ID', 'size_df2']\n",
    "df3_ranks.columns = ['HuMAP3_ID', 'size_df3']\n",
    "\n",
    "# Add rank columns\n",
    "df2_ranks['rank'] = range(1, len(df2_ranks) + 1)\n",
    "df3_ranks['rank'] = range(1, len(df3_ranks) + 1)\n",
    "\n",
    "# Merge the rankings side by side\n",
    "correspondence = pd.merge(\n",
    "    df2_ranks,\n",
    "    df3_ranks,\n",
    "    on='rank',\n",
    "    how='outer'\n",
    ")\n",
    "# Add details from the correspondences\n",
    "def get_correspondence_details(row):\n",
    "    if pd.notna(row['HuMAP2_ID']) and row['HuMAP2_ID'] in correspondences_dict:\n",
    "        details = correspondences_dict[row['HuMAP2_ID']]\n",
    "        return pd.Series({\n",
    "            #'HuMAP3_ID': details['HuMAP3_ID'],\n",
    "            #'size_df3': details['size_df3'],\n",
    "            'shared_items': details['shared_items'],\n",
    "            'improvement_in_v3': int(details['size_df3'] - row['size_df2'])\n",
    "        })\n",
    "    return pd.Series({\n",
    "        #'HuMAP3_ID': '',\n",
    "        #'size_df3': 0,\n",
    "        'shared_items': 'na',\n",
    "        'improvement_in_v3': 'na'\n",
    "    })\n",
    "\n",
    "correspondence_details = correspondence.apply(get_correspondence_details, axis=1)\n",
    "correspondence = pd.concat([correspondence, correspondence_details], axis=1)\n",
    "# Calculate coverage ratio\n",
    "def cov_ratio(row):\n",
    "    if isinstance(row['shared_items'], (int, float)) and row['size_df2'] > 0 and row['size_df3'] > 0:\n",
    "        return round(row['shared_items'] / min(row['size_df2'], row['size_df3']) * 100, 2) \n",
    "    else:\n",
    "        return 'na'\n",
    "\n",
    "correspondence['coverage_ratio'] = correspondence.apply(cov_ratio, axis=1)\n",
    "\n",
    "# Prepare display\n",
    "correspondence = correspondence.sort_values('rank')\n",
    "correspondence['size_df2'] = correspondence['size_df2'].fillna(0).astype(int)\n",
    "correspondence['size_df3'] = correspondence['size_df3'].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# Create display dataframe\n",
    "display_df = correspondence.copy()\n",
    "display_df['HuMAP2_ID'] = display_df['HuMAP2_ID'].fillna('')\n",
    "display_df['size_df2'] = display_df['size_df2'].replace(0, '')\n",
    "\n",
    "# Update print statement\n",
    "print(\"\\nCorrespondences (considering shared complex members) for version 2 vs. 3 with improvement, shared items, and coverage ratio:\")\n",
    "print(display_df[['HuMAP2_ID', 'size_df2', 'rank', 'HuMAP3_ID', 'size_df3', 'improvement_in_v3', 'shared_items', 'coverage_ratio']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4886c09-efbe-4c9e-b0fe-fa34d1c0d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disappearing Items between HuMAP2 and HuMAP3:\n",
      "\n",
      "HuMAP2 ID: HuMAP2_01834 -> HuMAP3 ID: huMAP3_12042.1\n",
      "Total items in HuMAP2: 5\n",
      "Total items in HuMAP3: 56\n",
      "Percent of items lost: 20.00%\n",
      "UniProt ACCs Lost From Corresponding Complex:\n",
      "  - P50993 (AND NOT IN ANY OTHER Hu.MAP3.0 ROGDI-complexes!!!)\n",
      "\n",
      "HuMAP2 ID: HuMAP2_03388 -> HuMAP3 ID: huMAP3_10511.1\n",
      "Total items in HuMAP2: 4\n",
      "Total items in HuMAP3: 35\n",
      "Percent of items lost: 25.00%\n",
      "UniProt ACCs Lost From Corresponding Complex:\n",
      "  - Q8TDJ6 (But present in other, ROGDI-related Hu.MAP3.0 complexes)\n"
     ]
    }
   ],
   "source": [
    "# Make sure nothing present in HuMAP2 data disappeared in huMAP3 data\n",
    "# Function to compare UniProt ACCs between HuMAP2 and HuMAP3\n",
    "def check_disappearing_items(df_correspondences, df2_expanded, df3_expanded):\n",
    "    # Tracking disappearing items\n",
    "    disappearing_items = []\n",
    "    \n",
    "    for _, correspondence in df_correspondences.iterrows():\n",
    "        humap2_id = correspondence['HuMAP2_ID']\n",
    "        humap3_id = correspondence['HuMAP3_ID']\n",
    "        \n",
    "        # Get UniProt ACCs for this HuMAP2 group\n",
    "        humap2_accs = set(df2_expanded[df2_expanded['HuMAP2_ID'] == humap2_id]['Uniprot_ACCs'])\n",
    "        \n",
    "        # Get UniProt ACCs for this HuMAP3 group\n",
    "        humap3_accs = set(df3_expanded[df3_expanded['HuMAP3_ID'] == humap3_id]['Uniprot_ACCs'])\n",
    "        \n",
    "        # Find items in HuMAP2 that are not in HuMAP3\n",
    "        lost_items = humap2_accs - humap3_accs\n",
    "        \n",
    "        if lost_items:\n",
    "            disappearing_items.append({\n",
    "                'HuMAP2_ID': humap2_id,\n",
    "                'HuMAP3_ID': humap3_id,\n",
    "                'lost_items': lost_items,\n",
    "                'total_humap2_items': len(humap2_accs),\n",
    "                'total_humap3_items': len(humap3_accs),\n",
    "                'percent_lost': len(lost_items) / len(humap2_accs) * 100 if humap2_accs else 0\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame for easy analysis\n",
    "    df_disappearing = pd.DataFrame(disappearing_items)\n",
    "    \n",
    "    # Display results\n",
    "    global something_dropped_completely\n",
    "    something_dropped_completely = False\n",
    "    if not df_disappearing.empty:\n",
    "        print(\"\\nDisappearing Items between Hu.MAP2.0 and Hu.MAP3.0 data:\")\n",
    "        for _, row in df_disappearing.iterrows():\n",
    "            print(f\"\\nHuMAP2 ID: {row['HuMAP2_ID']} -> HuMAP3 ID: {row['HuMAP3_ID']}\")\n",
    "            print(f\"Total items in HuMAP2: {row['total_humap2_items']}\")\n",
    "            print(f\"Total items in HuMAP3: {row['total_humap3_items']}\")\n",
    "            print(f\"Percent of items lost: {row['percent_lost']:.2f}%\")\n",
    "            print(\"UniProt ACCs Lost From Corresponding Complex:\")\n",
    "            for item in row['lost_items']:\n",
    "                dropped_completely = (item not in df3_expanded['Uniprot_ACCs'].to_list())\n",
    "                if dropped_completely:\n",
    "                    print(f\"  - {item} (AND NOT IN ANY OTHER Hu.MAP3.0 {search_term}-complexes!!!)\")\n",
    "                    something_dropped_completely = True\n",
    "                else:\n",
    "                    print(f\"  - {item} (But present in other, {search_term}-related Hu.MAP3.0 complexes)\")\n",
    "        \n",
    "        # Optional: Save to CSV\n",
    "        df_disappearing.to_csv('disappearing_items.csv', index=False)\n",
    "        \n",
    "        return df_disappearing, something_dropped_completely\n",
    "    else:\n",
    "        print(\"\\nNo disappearing items found between HuMAP2 and HuMAP3.\")\n",
    "        return None, something_dropped_completely\n",
    "\n",
    "# Call the function\n",
    "disappearing_df, something_dropped_completely = check_disappearing_items(df_correspondences, df2_expanded, df3_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "924889b2-8e87-44a8-aee0-0b4e0605ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hu.MAP2.0-Majority Complex Members Disappearing in Hu.MAP3.0 data:\n",
      "NONE\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Also make sure nothing that is in the majority of complexes in hu.MAP 2.0 data disappears entirely from hu.MAP 3.0 data.\n",
    "# That can only occur as the case if `something_dropped_completely`, that we've already check on, is True\n",
    "majority_item_dropped_completely = False #set up by declaring false\n",
    "if something_dropped_completely:\n",
    "    # collect information on what identifiers occur in the majority of complexes in hu.MAP 2.0 data\n",
    "    # the search term should always be present and can thus serve as a sanity check for this collection\n",
    "    majority_identifiers = []\n",
    "    def find_majority_uniprot_accs(df):\n",
    "        # Group by HuMAP2_ID and get unique Uniprot_ACCs for each complex\n",
    "        complex_accs = df.groupby('HuMAP2_ID')['Uniprot_ACCs'].unique()\n",
    "        \n",
    "        # Count total number of unique complexes\n",
    "        total_complexes = len(complex_accs)\n",
    "        \n",
    "        # Flatten and count occurrences of each Uniprot ACC across complexes\n",
    "        all_accs = df['Uniprot_ACCs'].tolist()\n",
    "        acc_counts = pd.Series(all_accs).value_counts()\n",
    "        \n",
    "        # Find ACCs that appear in more than 50% of complexes\n",
    "        majority_accs = acc_counts[acc_counts >= 0.51 * total_complexes].index.tolist()\n",
    "        \n",
    "        return majority_accs\n",
    "    if find_majority_uniprot_accs(df2_expanded):\n",
    "        majority_identifiers = find_majority_uniprot_accs(df2_expanded)\n",
    "    # Now check none of those `majority_identifiers` match the disappearing ones dropped completely\n",
    "    print(\"\\nHu.MAP2.0-Majority Complex Members Disappearing in Hu.MAP3.0 data:\")\n",
    "    for item in list(set.union(*disappearing_df['lost_items'])): # note `list(set.union(*disappearing_df['lost_items']))` gets the unique set members from the 'lost_items' column\n",
    "        if item in majority_identifiers:\n",
    "            majority_item_dropped_completely = (item not in df3_expanded['Uniprot_ACCs'].to_list())\n",
    "            if majority_item_dropped_completely:\n",
    "                print(f\"  - {item} IS IN MAJORITY OF Hu.MAP2.0 COMPLEXES YET NOT IN ANY Hu.MAP3.0 {search_term}-complexes!!! (PERHAPS CONCERNING?)\")\n",
    "                majority_item_dropped_completely = True\n",
    "    if majority_item_dropped_completely == False:\n",
    "        print(\"NONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3c792-5354-4f62-9f26-17ade69b3e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cc076-153f-4aa3-b0f1-e9ccc26508f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12417b75-689f-4a2d-851c-f5c19df57f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834d786-60c9-4dc3-a0aa-5b38d23537b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8251766-56fc-464f-b3cd-f5fc834832ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac854e73-9384-4c72-920e-87f2de6aca1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c273ae1-068d-46b2-b1df-2782ee15cdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuMAP2_IDs without correspondences:\n",
      "Empty DataFrame\n",
      "Columns: [HuMAP2_ID, size_df2]\n",
      "Index: []\n",
      "\n",
      "HuMAP3_IDs without correspondences:\n",
      "         HuMAP3_ID  size_df3\n",
      "3   huMAP3_09477.1        26\n",
      "4   huMAP3_09873.1        24\n",
      "5   huMAP3_08678.1        22\n",
      "6   huMAP3_09242.1        20\n",
      "7   huMAP3_07329.1        20\n",
      "8   huMAP3_08381.1        20\n",
      "9   huMAP3_07099.1        18\n",
      "10  huMAP3_01501.1        15\n",
      "11  huMAP3_01899.1        11\n",
      "12  huMAP3_03469.1         4\n",
      "13  huMAP3_05952.1         2\n"
     ]
    }
   ],
   "source": [
    "# PROBABLY DON'T NEED THIS BUT RECORDING IN CASE LATER I DO\n",
    "# Show IDs that don't have correspondences\n",
    "print(\"\\nHuMAP2_IDs without correspondences:\")\n",
    "print(correspondence[correspondence['HuMAP3_ID'].isna()][['HuMAP2_ID', 'size_df2']])\n",
    "\n",
    "print(\"\\nHuMAP3_IDs without correspondences:\")\n",
    "print(correspondence[correspondence['HuMAP2_ID'].isna()][['HuMAP3_ID', 'size_df3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae8d45-c051-4833-9939-93fe482a7764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16899e5-3641-4079-8386-16f0f78c998c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e994b2-e8f2-4d91-9db6-3dd2c10d7304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd544d5-fb3a-468a-81c5-22339ebca28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def executeSomething():\n",
    "    #code here\n",
    "    print ('.')\n",
    "    time.sleep(480) #60 seconds times 8 minutes\n",
    "\n",
    "while True:\n",
    "    executeSomething()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3e9d1-4740-4c91-ae20-fc9df5ba3ce8",
   "metadata": {},
   "source": [
    " You can change the hard-coded values here to investigate the differences for others. However, chances are you are interested in the differnces for several genes/proteins, and so you'll want to first check out the next notebook in this series, ['Using snakemake to highlight differences between hu.MAP 2.0 and hu.MAP 3.0 data for multiple identifiers'](Using_snakemake_to_highlight_differences_between_hu.MAP2_and_hu.MAP3_data_for_multiple_indentifiers.ipynb). That notebook will work through the steps to run a Snakemake workflow to process the identifiers for many proteins/genes to generate summary reports highlighting differences in human complexes reported in hu.MAP 2.0 vs. hu.MAP 3.0 data.\n",
    " \n",
    "-----\n",
    "\n",
    "Enjoy!\n",
    "\n",
    "See my [humap3-binder repo](https://github.com/fomightez/humap3-binder) and [humap3-utilities](https://github.com/fomightez/structurework/humap3-utilities) for related information & resources for this notebook.\n",
    "\n",
    "\n",
    "\n",
    "-----\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
